<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Stanford CS 231m - Yang Yan
  </title>

  <meta name="description" content="Part of Notes for Course Mobile Computer Vision with Additional Suppliments" /><meta name="generator" content="Hugo 0.121.1">

  <link rel="stylesheet" href="/main.css" />

  
  
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
      integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv"
      crossorigin="anonymous"
    >

    <script defer
      src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
      integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
      crossorigin="anonymous"
    ></script>

    <script defer
      src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
      integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);">
    </script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
    </script>
  

  <meta property="og:title" content="Stanford CS 231m" />
<meta property="og:description" content="Part of Notes for Course Mobile Computer Vision with Additional Suppliments" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yan99.github.io/notes/cs231m/" /><meta property="article:section" content="Notes" />





  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Stanford CS 231m"/>
<meta name="twitter:description" content="Part of Notes for Course Mobile Computer Vision with Additional Suppliments"/>


  <meta itemprop="name" content="Stanford CS 231m">
<meta itemprop="description" content="Part of Notes for Course Mobile Computer Vision with Additional Suppliments">

<meta itemprop="wordCount" content="3496">
<meta itemprop="keywords" content="" />

  <meta itemprop="name" content="Stanford CS 231m">
<meta itemprop="description" content="Part of Notes for Course Mobile Computer Vision with Additional Suppliments">

<meta itemprop="wordCount" content="3496">
<meta itemprop="keywords" content="" />
</head><body class="flex relative h-full min-h-screen"><aside
  class="will-change-transform transform transition-transform -translate-x-full absolute top-0 left-0 md:relative md:translate-x-0 w-3/4 md:basis-60 h-full min-h-screen p-3 bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 flex flex-col gap-2.5 z-20 sidebar flex-shrink-0">
  <p class="font-bold mb-5 flex items-center gap-2">
    <button aria-label="Close sidebar"
      class="md:hidden menu-trigger-close p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-6 w-6" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
</svg></button>
    <a href="https://yan99.github.io/" class="px-2">
      <span>Yang Yan</span>
    </a>
    <button aria-label="Toggle dark mode"
      class="dark-mode-toggle p-2 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <circle cx="12" cy="12" r="4" />
  <path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" />
</svg></button>
  </p>

  
  <ul class="list-none flex flex-col gap-1">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/" >
        <span>Home</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none "
        href="#" >
        <span>Content</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/notes/" >
        <span>Course Notes</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/othernotes/" >
        <span>Other Notes</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/projects/" >
        <span>Projects</span>
        
      </a>
    </li>
    
  </ul>

  <div class="flex-1"></div>

  

  <ul class="list-none flex flex-wrap justify-center gap-1 pt-2 border-t border-slate-200 dark:border-slate-600">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">Github</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">LinkedIn</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z" />
  <rect x="2" y="9" width="4" height="12" />
  <circle cx="4" cy="4" r="2" />
</svg></span>
        
      </a>
    </li>
    
  </ul>
</aside>

<div
  class="fixed bg-slate-700 bg-opacity-5 transition duration-200 ease-in-out inset-0 z-10 pointer-events-auto md:hidden left-0 top-0 w-full h-full hidden menu-overlay">
</div>

<button aria-label="Toggle Sidebar"
  class="md:hidden absolute top-3 left-3 z-10 menu-trigger p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-100"><svg class="h-6 w-6" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="4" y1="6" x2="20" y2="6" />
  <line x1="4" y1="12" x2="20" y2="12" />
  <line x1="4" y1="18" x2="16" y2="18" />
</svg></button>





<div class="flex max-h-screen relative overflow-y-auto h-full w-full">
  
  <article class="px-6 py-20 w-full mx-auto prose lg:prose-lg dark:prose-invert h-fit prose-img:mx-auto">
    
    <a href="https://yan99.github.io/notes/" class="!no-underline !text-slate-500 !mb-2 !inline-block !font-normal !text-sm hover:!text-slate-700 dark:hover:!text-slate-400">← Back to list</a>

    
    <h1 class="!mb-2">Stanford CS 231m</h1>
    

    

    <h2 id="panoramas">Panoramas</h2>
<p>Can be achieved with wide-angle optics or rotation cameras.</p>
<h3 id="system-overview">System Overview</h3>
<ul>
<li>Camera Module -&gt; Video Frames -&gt; Real-Time Tracking -&gt; Camera Module</li>
<li>Camera Module -&gt; Images -&gt; Warping -&gt; Registration -&gt; Blending -&gt; Final Panorama</li>
</ul>
<h3 id="cylindrical-panoramas">Cylindrical Panoramas</h3>
<ul>
<li>Project each image onto a cylinder</li>
<li>A cylindrical image is a rectangular array</li>
<li>View: reproject a portion of the cylinder onto a picture plane representing the display screen</li>
<li>Narrow FOV -&gt; less distortion</li>
<li>Same center of projection.</li>
</ul>
<h3 id="stitching">Stitching</h3>
<p>Detect key points -&gt; find corresponding pairs -&gt; align images</p>
<h4 id="key-points">Key Points</h4>
<ul>
<li>Harris corner detector: $E(u,v) = \sum_{x, y}w(x,y)[I(x+u,y+v)-I(x,y)]^{2}\simeq\begin{bmatrix}u &amp; v\end{bmatrix}M\begin{bmatrix}u \\ v\end{bmatrix}$, where $w(x,y)$ is window function ($1$ in window, $0$ outside or Gaussian)</li>
<li>Measure of corner response: $M = \sum_{x,y}w(x,y)\begin{bmatrix} I_{x}^{2} &amp; I_{x}I_{y} \\ I_{x}I_{y} &amp; I_{y}^{2} \end{bmatrix}$</li>
<li>Both $\lambda_{1}$ and $\lambda_{2}$ are not too large</li>
<li>Another way with Haar instead of window function: $I_{x}$ and $I_{y}$ are x-oriented and y-oriented Haar filters at scale $\sigma$ and $M = \begin{bmatrix} \sum I_{x}^{2} &amp; \sum I_{x}I_{y} \\ \sum I_{x}I_{y} &amp; \sum I_{y}^{2} \end{bmatrix}$ over all the pixels in $5\sigma\times5\sigma$ neighborhood</li>
<li>$R = det(M) - k(trace(M))^{2}$, $k \in [0.04,0.06]$, $R&gt;0$ but not small</li>
<li>Harris corner detector Workflow: corner response $R$ -&gt; thresholding -&gt; local maxima</li>
<li>FAST (features from accelerated segment test) corners: look for a continuous arc of N pixels, all much darker/brighter than center pixel
<ul>
<li>Normally, radius is $3$, $N = 16$</li>
<li>For fast implementation: check pixel intensities of pixel $1$, $5$, $9$, and $13$ with the center pixel intensity. Then, check the rest of the $12$ pixels.</li>
<li>Limitations: when $N&lt;12$, the detection rate is too high</li>
</ul>
</li>
</ul>
<h4 id="point-descriptors">Point Descriptors</h4>
<p>Invariant and distinctive</p>
<ul>
<li>SIFT (Scale Invariant Feature Transform)
<ul>
<li>Locate high-variance interest points, representing them with a vector attribute of local gray level variations</li>
<li>DoG pyramid:
<ol>
<li>1 octave above the level before</li>
<li>Each octave has images of same size</li>
<li>Within octave, apply same Gaussian</li>
<li>Downsample and upsample to restone resolution</li>
</ol>
</li>
<li>Local extreme in DoG for each image ($D(x, y, \sigma)$)
<ol>
<li>Local extreme among current and immediate recent $3x3x3$ pixels
<ul>
<li>It is recommended that finding the extrema for $&gt;= 2$ different values of scale $\sigma$ in each octave</li>
<li>-&gt; $&gt;=4$ values for $\sigma$ -&gt; At least 5 different scale Gaissuan images in each octave</li>
</ul>
</li>
<li>True locationn based on Taylor&rsquo;s expansion: $\overrightarrow{x} = - H^{-1}(\overrightarrow{x_{0}})\cdot \overrightarrow{J}(\overrightarrow{x_{0}})$</li>
<li>Threshold out the weak extrema: $|D(\overrightarrow{x})| &lt; 0.03$</li>
</ol>
</li>
<li>Dominant local orientation
<ol>
<li>Gradient magnitude at local extreme location on Gaussian-smoothed image: $$m(x,y) = \sqrt{|ff(x+1,y,\sigma) - ff(x, y, \sigma)|^{2}  + |ff(x, y + 1, \sigma) - ff(x, y, \sigma)|^{2}}$$</li>
<li>Gradient orientation: $\theta(x,y) = \arctan \frac{ff(x, y + 1, \sigma) - ff(x,y,\sigma)}{ff(x+1, y, \sigma)-ff(x,y,\sigma)}$</li>
<li>Weight $\theta(x,y)$ with $m(x,y)$</li>
<li>Construct a histogram of $\theta(x,y)$ with 36 bins for 360 degree, the histogram peak/fitted parabola gives the dominant local orientation.</li>
</ol>
</li>
<li>128-dimentional vector
<ol>
<li>Grdient magnitude is the same as previous step; Gradient directions are measured relative to the dominant local orientation</li>
<li>At the scale of the extremum, the extreme point is surrounded by a $16\times16$ neighborhood of points ($4\times4$ cells, each cell contains $4\times4$ block of points). Magnitudes are weighted by a Gaussian where $\sigma$ is $1/2$ width of the neighborhood to reduce the importance of the points that are relatively far</li>
<li>For each of the $16$ cells, and 8-bin orientation histogram is caluclated based on gradient-magnitude-weighted values of $\theta(x,y)$ at the $16$ pixels in the cell.</li>
<li>Concat the $16$ 8-bin histograms yields a 128-element descriptor</li>
<li>128-element descriptor for each extremum in DoG -&gt; normalized to unity for this 128 elements -&gt; invariant to illumination</li>
</ol>
</li>
</ul>
</li>
<li>Blob detection in 2D
<ul>
<li>Laplacian of Gaussian (LoG): Circularly symmetric operator for blob detection in 2D.</li>
<li>$\Delta^{2}_{norm}g =\sigma^{2}(\frac{\partial^{2}g}{\partial x^{2}}+\frac{\partial^{2}g}{\partial y^{2}})$, where $\frac{\partial^{2}g}{\partial x^{2}}=g(x-1, y)-2g(x,y)+g(x+1,y)$</li>
<li>Or with filter kernel $\begin{bmatrix}0&amp;1&amp;0\\ 1&amp;-4&amp;1\\ 0&amp;1&amp;0 \end{bmatrix}$</li>
</ul>
</li>
<li>SURF (Speeded Up Robust Features)
<ul>
<li>Interest point where determinant of Hessian is maximized (determinant of Hessian measures Guassian curvature of surface; Gaussian curvature$\uparrow$, strong variance in every direction)</li>
<li>Differences: integral images for fast caluclation of derivatives; only concept of pyramids; No downsampling; Resolution stays same</li>
<li>Integral images: $I(x,y) = \sum_{x}\sum_{y}f(i,j)$</li>
<li>Discretization of $\frac{\partial^{2}}{\partial x^{2}}ff(x,y)$ in the Hessian
<ol>
<li>Convolve the image $f(x,y)$ with $\frac{\partial^{2}g}{\partial x^{2}}$, where $g(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}}{2\sigma^{2}}}$ -&gt; actually, we need to convolve with half-width central lobe $h(x) = -\frac{1}{\sqrt{2\pi}\sigma^{3}}[1-\frac{x^{2}}{\sigma^{2}}]e^{-\frac{x^{2}}{2\sigma^{2}}}$</li>
<li>Descrete 2D operators: The interest points are located at the pixels where the determinant is a maximum with respect $x$, $y$, and $\sigma$
/<img src="/images/9x9_Operator.png" alt="drawing" width="25%"/></li>
<li>Calculate first-order derivatives $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}$ with Haar wavelet filters.
<ul>
<li>Basic forms of Haar wavelet: $\begin{bmatrix}-1 &amp; 1\end{bmatrix}$ along $x$ and $\begin{bmatrix}-1 \\ 1\end{bmatrix}$ along $y$.</li>
<li>Scale up to $M\times M$ operator ($M$ is even; $M&gt;4\sigma$)</li>
<li>Could use integral image $I(x,y)$</li>
</ul>
</li>
<li>Local dominant direction
<ul>
<li>Range: $6\sigma\times 6\sigma$ neighborhood</li>
<li>$(d_{x}, d_{y})$ are weighted with $2\sigma$-Gaussian centered at interest points</li>
<li>Scanning a scatter plot of the weighted $(d_{x}, d_{y})$ with a $60\deg$ cone. Largest resultant vector.</li>
</ul>
</li>
<li>Descriptor at scale $\sigma$
<ul>
<li>Range: $20\sigma\times 20\sigma$</li>
<li>Orientation: local dominant direction</li>
<li>$20\sigma\times 20\sigma$ neighborhood is divided into a pattern of $4\times4$ squares</li>
<li>Each squares (containing $5\sigma\times 5\sigma$ points) construct a $4$-vector $(\sum d_{x}&rsquo;, \sum d_{x}&rsquo;, \sum |d_{x}&rsquo;|, \sum |d_{x}&rsquo;|)$, where $d_{x}&rsquo;$ and $d_{y}&rsquo;$ are outputs of the two Haar operators relative to the lcoal dominant direction.</li>
<li>Total $64$-element descriptor vector at each interest point after concatnating $16$ $4$-vectors.</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>ORB (Oriented FAST and Rotated BRIEF)
<ul>
<li>Use FAST-9 (Harris)</li>
<li>Find orientation: calculate weighted new center $\frac{\sum xI(x,y)}{\sum I(x,y)}, \frac{\sum yI(x,y)}{\sum I(x,y)}$</li>
<li>Reorient image so that gradients vary vertically</li>
<li>BRIEF (Binary Robust Independent Element Features): Gaussian smooth first; randomly choose a pair of pixels in a defined neighborhood around the key point (1st $\sigma$, 2nd $\sigma/2$); compare the pair of pixels intensities getting 1 or 0; Randomly choose pairs for $N$ times for $N$-bit vector; rotated BRIEF (rBRIEF) according to the patch orientation $R_{\theta}\times Q$, where $R$ is rotation matrix, and $Q$ is of shape $1\times N$</li>
<li>Feature matching: compare the binary vectors with Hamming distance(XOR + pop count)</li>
</ul>
</li>
<li>SuperPoint Self-Supervision Networks</li>
</ul>
<h4 id="feature-matching">Feature Matching</h4>
<ul>
<li>SAD (Sum of Absolute Differences): $\min \sum\sum|f_{1}(i,j) - f_{2}(i,j)|$</li>
<li>SSD (Sum of Squared Differences): $\min \sum\sum|f_{1}(i,j)-f_{2}(i,j)|^{2}$</li>
<li>NCC (Nomalized Cross-Correlation): $\max \frac{\sum f_{1}(i,j)f_{2}(i,j)}{\sqrt{\sum f_{1}(i,j)^{2}\sum f_{2}(i,j)^{2}}}$</li>
</ul>
<h4 id="aligning-images">Aligning Images</h4>
<ul>
<li>Homography = planar propjective transformation
<ul>
<li>Linear transformation on homogeneous $3$-vectors, the transformation being represented  by a non-singular $3\times3$ matrix $H$</li>
<li>Point $x$: $x&rsquo; = Hx$</li>
<li>Line $l$: $l^{T}x_{i} = 0$ and $x_{i}&rsquo; = Hx_{i}\implies l&rsquo;=H^{-T}l$</li>
<li>Conics $C$: $x^{T}Cx = 0$ and $x = H^{-1}x&rsquo;\implies C&rsquo;=H^{-T}CH^{-1}$</li>
<li>Maps a traight line to a straight line</li>
<li>Projective Group $\rightarrow$ Affine Group $\rightarrow$ Similarity Group $\rightarrow$ Euclidean Group
<ol>
<li>$H = \begin{bmatrix} A &amp; \vec{t}\\\vec{v^{T}} &amp; \nu \end{bmatrix}$</li>
<li>Affine: $\begin{bmatrix} 0&amp;0&amp;1 \end{bmatrix}$ for last row of $H$, keeps parallel lines parallel</li>
<li>Similarity: $A^{T}A = \lambda^{2}I$ ($A$ is orthogonal), shpaes preserved</li>
<li>Euclidean: $A^{T}A = I$ ($A$ is orthonormal), rigid body motion, preserve Euclidean distance</li>
</ol>
</li>
<li>Affine transform maps $l_{\infty}$ to $l_{\infty}=\begin{bmatrix} 0&amp;0&amp;1 \end{bmatrix}^{T}$</li>
<li>General projective transform maps $l_{\infty}$ to vanishing line.</li>
<li>By sending vanishing line to $l_{\infty}$ can remove affine distortion.</li>
</ul>
</li>
<li>$x&rsquo; = Hx$ for each corresponding point pairs. Rearrange the equations. $\rightarrow Ah = 0$, where $A_{i} = \begin{bmatrix} -x&amp;-y&amp;-1&amp;0&amp;0&amp;0&amp;xx&rsquo;&amp;yx&rsquo;&amp;x&rsquo;\\0&amp;0&amp;0&amp;-x&amp;-y&amp;-1&amp;xy&rsquo;&amp;yy&rsquo;&amp;y&rsquo;\end{bmatrix}$. Decomposite $A$ with SVD. Solution is the nullspace of A, the last column/row of $V$/$V^{T}$. Or solve the least square problem of $min|Ah|$ with $|h| = 1$. Or set $h_{3,3} = 1$, solve least square problem of $|Ah-b|$ with solutioin of $h = (A^{T}A)^{-1}A^{T}b$</li>
<li>RANSAC (Random Sample Consensus)
<ol>
<li>Randomly choose a subset of data points to fit</li>
<li>Within a threshold $\delta$ of model are consensus set (inliers). No. of inliers is $M$.</li>
<li>Repeat for $N$ times. The model with biggest $M$ is the most robust fit</li>
</ol>
<ul>
<li>Assume $\epsilon = 10%$ outliers, goal: the probability $p$ for at least $1$ sample does not include any outliers
For the case, $&gt;1$ outliners included in current sample, the probability is $1-(1-\epsilon)^{n}$, where $n$ is no. of point pairs
For $N$ samples, at least $1$ sample is free of outliers, the probaility is $1-p = [1-(1-\epsilon)^{n}]^{N}\rightarrow N = \log_{1-(1-\epsilon)^{n}}{(1-p)}$</li>
</ul>
</li>
<li>Hybrid Multi-Resolution Registration
<ul>
<li>Coarse-to-fine strategy: Pyramid from low resolution (downsampled) to high resolution (Upsampled)</li>
<li>Image based registration for initial guess</li>
<li>Feature based registration: feature detection (Harris corners) $\rightarrow$ feature matching $\rightarrow$ RANSAC$\rightarrow$ New estimate</li>
</ul>
</li>
</ul>
<h3 id="blending">Blending</h3>
<p>Directly averaging the overlapped pixels results in ghosting artifacts (moving objects, errors in registration, parallax, etc.)</p>
<h4 id="alpha-blendingfeathering-i_blend--alpha-i_left--1-alphai_right">Alpha blending/feathering: $I_{blend} = \alpha I_{left} + (1-\alpha)I_{right}$</h4>
<h4 id="issues">Issues</h4>
<ul>
<li>Drifing
<ul>
<li>Vertical error accumulation &lt;- Apply correction make zero sum in vertical displacement</li>
<li>Horizontal error accumulation &lt;- Reuse 1st/last image for right panorama radius</li>
</ul>
</li>
<li>Ghosting
<ul>
<li>Assign one input image to each output pixel (graph cut)</li>
<li>Introducing new artifacts: inconsistency between pixels from images caused by exposure/white balance/vignetting</li>
<li>Poisson blending: copy gradient field from inputimage, reconstruct the final image with Poisson equation ($E = \sum s^{x}[I_{x}-g_{x}]^{2} + s^{y}[I_{y}-g_{y}]^{2} + wI^{2}$, where $g_{x}$ and $g_{y}$ is the target gradient values, $s^{x}$ and $s^{y}$ are smoothness weights, $w$ is panelized parameter for encourage/disencourage to go back to original pixel value) in the coarse multi-spline domain.</li>
<li>Laplacian pyramid blending:
<ol>
<li>$L_{A}$ and $L_{B}$ are Laplacian pyramids of images $A$ and $B$. (Laplacian: Gaussian-expanded deeper level of Gaussian)</li>
<li>Gaussian pyramid $G_{M}$ for selection mask $M$</li>
<li>The combined pyramid $L_{S}$: $L_{S} = G_{M}\cdot L_{A}+(1-G_{M})\cdot L_{B}$</li>
<li>Collapse the $L_{S}$ pyramid to get the final blended image.</li>
</ol>
</li>
<li>Multi-resolution fusion: two images with different exposure time, use Guassian pyramid as weight matrix weight Laplacian pyramid to get a fused Laplacian pyramid to get a final image.</li>
<li>Two-band blening: high freq and low freq.</li>
</ul>
</li>
</ul>
<h2 id="hdr-imaging">HDR Imaging</h2>
<h3 id="dynamic-range">Dynamic Range</h3>
<ul>
<li>Luminance: a photometric measure of the luminuous intensity per unit area of light travelling in a given direction (unit: candela per square metre $cd/m^{2} = lux$)
* Eye can adapt from $10^{-6}$ to $10^{8} cd/m^{2}$
* without adaptation from $1$ to $10^{4} cd/m^{2}$
* with non-specular reflectance from $1$ to $10^{3} cd/m^{2}$
* Display: $1$ to $100 cd/m^{2}$ (0-255)</li>
</ul>
<h3 id="hdr-previous-methods">HDR Previous Methods</h3>
<ul>
<li>Human HDR: sensitive to contrast (log scale); pupil; neural &amp; chemical; transmition to brain</li>
<li>Multiple exposure photography: map segments of high dynamic range to low contrasts</li>
<li>Camera response curve calibration for the multiple images with various exposure time: adjust radiance to obtain a smooth response curve</li>
<li>Dodging and burning: hide partial during exposure; exposure more to a region; smooth circular motions &amp; blurry mask avoid artifacts</li>
<li>Gamma correction</li>
<li>Global tone mapping (various contrast for each image)</li>
<li>Local tone mapping (Reinhard operator to avoid high light satrurated)</li>
<li>Hitogram adjustment: lum not even $\rightarrow$ equalization, histogram in log space $\rightarrow$ issue: expand comtrast $\rightarrow$ trimming large bins (adjustment)</li>
<li>Oppenheim: low contrast on low-freq, keep high-freq $\rightarrow$ issue: halo $\rightarrow$ bilateral filtering on intensity</li>
<li>Exposure fusion: weights over 3 maps: Laplacian filter $\rightarrow$ contrast map; saturation on RGB respectively; exposure stretched based on gaussian distribution</li>
<li>Multi-resolution fusion: Laplacian pyramid $*$ Gaussian Pyramid generate fused pyramid</li>
<li>HDR video: auto exposure, motion compensation (registration between frames), tone mapping</li>
</ul>
<h3 id="system-overview-1">System Overview</h3>
<p>Reference frame selection $\rightarrow$ consistency detection $\rightarrow$ HDR generation $\rightarrow$ Poisson blending</p>
<h3 id="hdr-optical-system">HDR optical system</h3>
<ul>
<li>Beam splitting prism breaks up the light into three parts, one for each sensor fitted with different filters</li>
<li>With 2 beam splitters instead of prism to increase the light efficiency</li>
</ul>
<h2 id="camera-isp">Camera ISP</h2>
<h3 id="pinhole-camera">Pinhole Camera</h3>
<ul>
<li>Without pinhole, all the sensor points would record similar colors (integral of light from every point on subject)</li>
<li>Effect of pinhole size: large pinhole (geometric blur), optimal pinhole (too little light), small pinhole (diffraction blur)</li>
<li>Lens
<ul>
<li>Gauss&rsquo;s ray diagram</li>
<li>Focus distance: $\frac{1}{s_{o}}+\frac{1}{s_{i}} = \frac{1}{f}$</li>
<li>At $s_{o} = s_{i} = 2f$, get $1:1$ imaging</li>
<li>Depth of field: the range of distances that area in focus (can&rsquo;t focus on obejcts closer to the lens than $f$)</li>
<li>Chromatic aberration
<ul>
<li>Different wavelengths refract at different rates $\rightarrow$ different focal lengths</li>
<li>To align red and blue with achromatic doublet: Crown lens (strong positive lens) + Flint lens (weak negative lens)</li>
</ul>
</li>
<li>Lens distortion
<ul>
<li>Radial change in magnification: pincushion （corners stretching out）, barrel distortion (corners squeezing in)</li>
<li>Vignetting
<ul>
<li>Irradiance: radiant flux received by a surface per unit area. (Unit: $W\cdot m^{-2}$)</li>
<li>Radiant flux or radiant power: the radiant energy emitted, reflected, transmitted, or received per unit time                       * Irradiance is proportional to projected area of aperture as seen from pixel of image plane</li>
<li>Irradiance is proportional to projected area of pixel as seen from aperture</li>
<li>Irradiance is proportional to the distance^2 from aperture to pixel</li>
<li>Each of above factor $\approx \cos \theta$. Thus, light drops as $cos^{4}\theta$, $\theta$ is the angle measured from center of the aperture to the pixel versus horizontal line</li>
<li>Flat fielding: take a photo of a uniformly white object; resulting image shows attenuation</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="cmos-sensor">CMOS Sensor</h3>
<ul>
<li>Structures
<ul>
<li>Front-illuminated sensor: on-chip microlens $\rightarrow$ color filter $\rightarrow$ metal wiring $\rightarrow$ light receiving surface $\rightarrow$ Silicon substrate, photo-diode and potential well</li>
<li>Back-illuminated sensor: on-chip microlens $\rightarrow$ color filter $\rightarrow$ light receiving surface $\rightarrow$ Silicon substrate, photo-diode and potential well $\rightarrow$ metal wiring</li>
<li>Metal wiring includes: transistors (amplifier, row select bus, column bus, reset)</li>
<li>Anti-aliasing filter: 2 layers of birefrigent material to split one ray into 4 rays (birefrigence: a refractive index that depends on the polarization and propagation direction of light)</li>
<li>Front-illuminated sensor: similar to eyes</li>
<li>Back-illuminated sensor:
<ul>
<li>Cons: Cross talk or color mixing with adjacent pixels; silicon waffer fragile</li>
<li>Pros: Increase light; captured improve low-light performance</li>
</ul>
</li>
</ul>
</li>
<li>RAW
<ul>
<li>Pixel non-uniformity
<ul>
<li>Each pixel in a CCD has a slightly different sensitivity to light (within 1% to 2%)</li>
<li>Can be calibrated wiith a flat-field image</li>
<li>When calibrating with a flat-field image, the effects of vignetting eliminate as well as other optical variations</li>
</ul>
</li>
<li>Stuck pixels: some pixels are turned always on or off $\rightarrow$ identify, replace with filtered values</li>
<li>Black level substraction: temperature adds noise; sensors usually have a ring of covered pixels around the pixels that exposed to source to capture the noise floor, subtract their signal</li>
</ul>
</li>
<li>AD Conversion
<ul>
<li>Sensor converts analog light signal to analog electrical signal</li>
<li>AD Conversion: $\geq10$ bits (often $12$ or more); linear response; No. of bits = No. of op-amp</li>
<li>Op-amp: $V_{out} = A\cdot(V_{in})$; Note that for op-amp, $V_{in,+}\approx V_{in,-}$</li>
<li>ISO = amplification in AD conversion
<ul>
<li>Before AD conversion, the signal can be amplified</li>
<li>ISO 100 means no amplification; ISO 1600 means 16 amplification</li>
<li>$+$ can see details in dark areas better</li>
<li>$-$ noise is amplified as well; sensor more likely to saturate</li>
</ul>
</li>
</ul>
</li>
<li>Color Filter Array: Bayer Pattern
<ul>
<li>$\begin{bmatrix} \color{blue}B&amp; \color{green}G&amp; \color{blue}B&amp;\color{green}G \\ \color{green}G&amp;\color{red}R&amp;\color{green}G&amp;\color{red}R \\\color{blue}B&amp;\color{green}G&amp;\color{blue}B&amp;\color{green}G \end{bmatrix}$</li>
<li>$36$ megapixels contain $9$ megapixels of R, $18$ megapixels of G, and $9$ megapixels of B.</li>
<li>Demosaicking: separate RGB into 3 channels
<ul>
<li>Bilinear interpolation: easy to implement; fails at sharp edges</li>
<li>Dealing with edges: bilateral filtering</li>
<li>Dealing with edges: compare the red value to its estimate from the bilinear interpolation with the nearest 4 read pixels; add a proportion of this difference to the interpolation results for green value estimation at the location. The difference indicates change of luminance. $\hat{g}(i,j) = \hat{g}(i,j) + \alpha \Delta_{\hat{R}}(i,j)$
<img src="/images/Demosaicing.png" alt="drawing" width="35%"/></li>
</ul>
</li>
<li>Denoising using non-local means
<ul>
<li>Image self-similarity can be used to eliminate noise: average a group of squares which are almost indistinguishable</li>
<li>BM3D (Block Matching 3D)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="color-science-more-details-can-be-found--course-notespurdue-ece-638notesece638">Color Science (More Details can be found → <a href="/../notes/ece638" >Course Notes/Purdue ECE 638</a>)</h3>
<ul>
<li>CIE Chromaticity Diagram
<ul>
<li>Intensity is imeasured as the distance from origin black $(0,0,0)$</li>
<li>Chromaticiity coordinates given a notion of color independent of brightness</li>
<li>$x+y+z=1$ yields a chromaticity value dependent on dominant wavelength (hue) and excitation purity (saturation, distance from equal energy white point $(1/3,1/3,1/3)$)</li>
<li>Perceptural non-uniformity: the XYZ color space is not perceptually uniform (MacAdam ellipse, just noticeable difference)</li>
</ul>
</li>
<li>CIE L*a*b* uniform color space
<ul>
<li>Approximate human vision, L component closely matches human perception of lightness</li>
<li>a channel: red-green; b channel: blue-yellow</li>
</ul>
</li>
<li>YUV, YC<sub>b</sub>C<sub>r</sub>, &hellip;
<ul>
<li>Family of color spaces for video encoding</li>
<li>Eye is sensitive to luminance $\rightarrow$ separate luminance and chrominance</li>
<li>Get low frequency UV compressed; Filters color down (2:1, 4:1)</li>
<li>Channels: Y = luminance (linear); Y&rsquo; = luma (gamma corrected); UV/C<sub>b</sub>C<sub>r</sub> = chrominance (always linear)</li>
<li>Y&rsquo;C<sub>b</sub>C<sub>r</sub> is not an absolute color space (the actual color depends on the RGB primaries used)</li>
<li>YC<sub>b</sub>C<sub>r</sub> is YUV represented differently
<ul>
<li>YUV: luminance and BR luminance</li>
<li>YC<sub>b</sub>C<sub>r</sub> (scaled YUV): luminance, blue minus luminance (B-Y), and red minus luminance (R-Y)</li>
<li>YP<sub>b</sub>P<sub>r</sub> (physical YC<sub>b</sub>C<sub>r</sub>): analog version of YC<sub>b</sub>C<sub>r</sub>, physical component video cable used to transmit YC<sub>b</sub>C<sub>r</sub></li>
</ul>
</li>
</ul>
</li>
<li>Gamma encoding</li>
<li>Luminance from RGB
<ul>
<li>Green will appear the brightest, red will appear less bright, blue will be the darkest</li>
<li>For luminance by NTSE, CIE, ITU are different for RGB gain</li>
</ul>
</li>
<li>Cameras use sRGB
<ul>
<li>standard RGB color space (uses the same primaries as used in studio monitors and HDTV, gamma curve typical of CRTs, direct display)</li>
<li>Need a map from sensor RGB to sRGB, need calibration</li>
</ul>
</li>
<li>Linear or non-linear
<ul>
<li>linear: simulatinmg physical world</li>
<li>non-linear: HVS, minimizing perceptual errors due to quantization</li>
</ul>
</li>
<li>Film response curve (Density vs. Log exposure)
<ul>
<li>Toe region: chemical process</li>
<li>Middle linear region: a given amount of light turned half of the grain crystals to silver, the same amount more turns half of the rest</li>
<li>Shoulder region: close to saturation</li>
<li>Toe and shoulder preserve more dynamic range at dark and bright areas at the cost of reduced contrast</li>
<li>Film has more dynamic range than print (~12 bits)</li>
</ul>
</li>
<li>Digital camera response curve(Intensity vs. irradiance)
<ul>
<li>Irradiance: radiant flux received by a surface per unit area</li>
<li>Use different response curves at different exposures</li>
</ul>
</li>
</ul>
<h3 id="3a-more-details-can-be-found--other-notes3aothernotes3a">3A (More Details can be found → <a href="/../othernotes/3a" >Other Notes/3A</a>)</h3>
<ul>
<li>Auto focus
<ul>
<li>Filter pixels with configuragle IIR filters (infinite impulse response) to produce a low-resolution sharpness map of the image</li>
<li>The sharpness map helps estimate the best lens position by summing the sharpness values (=focus value) either over the entire image or over a rectangular area. (coarse to fine strategy)</li>
</ul>
</li>
<li>Auto white balance
<ul>
<li>Illuminant determines the color normally associated with white by HVS</li>
<li>RAW image has more green color</li>
<li>Identify the illuminant color $\rightarrow$ neutralize the color of the illuminant
<ul>
<li>Prior knowledge of illuminant color temperature</li>
<li>Known reference object in the picture: find some white or gray</li>
<li>Gray world assumption (gray in sRGB space)</li>
</ul>
</li>
<li>Methods
<ul>
<li>Grey card (a picture of a white or grey obejct, deduct the weight of each channel)</li>
<li>An object with known $(r_{w},g_{w},b_{w})$</li>
<li>Use brightest pixels that is not saturated/clipped, make it white</li>
</ul>
</li>
<li>Mapping the colors
<ul>
<li>For a given sensor: pre-compute the transformation matrices between the sensor color space and sRGB at different temperatures</li>
<li>For an unknown sensor: interpolating between pre-computed matrices</li>
</ul>
</li>
<li>Estimating color temperature
<ul>
<li>Use gray world assumption (make average of all pixels gray $R=G=B$) in sRGB space (just using $R=B$)</li>
<li>Estimate color temperature in a given image
<ol>
<li>Apply pre-computed matrix to get sRGB for $T_{1}$ and $T_{2}$</li>
<li>Calculate the average values $R$, $B$</li>
<li>Solve $\alpha$</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Auto exposure
<ul>
<li>Parameters: exposure time, aperture, analog and digital gain, ND (nertral density) filters (modifies intensites without change in hue)</li>
<li>Exposure metering: CDF of image intensity values, $P$ percentile of image pixels have an intensity lower than intensity $Y$; Separate highlights and shadows areas</li>
</ul>
</li>
</ul>
<h3 id="image-formatting">Image Formatting</h3>
<ul>
<li>JPEG encoding
<ol>
<li>RGB $\rightarrow$ YUV/YIQ and subsample color</li>
<li>DCT (discrete cosine transform) on $8\times8$ blocks</li>
<li>Quantization</li>
<li>Zig-zag ordering and run-length encoding</li>
<li>Entropy coding</li>
</ol>
<ul>
<li>Notes: Quantization tables and entropy coding tables are stored in between header and image data</li>
</ul>
</li>
<li>Other formats:
<ul>
<li>JPEG2000: ISO = $2000$; better compression, inherently hierachical, random access; much more complex</li>
<li>JPEG XR: Microsoft 2006; ISO/ITU-T, 2010; good compression, supports tiling (randomly access without having to decode whole image); better color accuracy  (including HDR); transparency; compressed domain editing</li>
</ul>
</li>
</ul>
<h3 id="camera-apis">Camera APIs</h3>
<ul>
<li>Traditional camera APIs
<ul>
<li>Real image sensors are pipelined: while one frame exposing, next frame is being prepared and previous frame is being read out.</li>
<li>Viewfinding/video mode: pipelined, high frame rate; settings changes take effect someime later</li>
<li>Still capture mode: paramteres prepared; reset pipeline between shots</li>
<li>Image sensor: (exposure, frame rate) configure $\rightarrow$ expose $\rightarrow$ (gain, digital zoom) readout</li>
<li>Imaging pipeline: (format) receive $\rightarrow$ (Coefficients) Demosaic $\rightarrow$ (white balance) color corr</li>
</ul>
</li>
<li>The FCam Architecture
<ul>
<li>A software architecture for programmable cameras: expose the maximum device capabilities</li>
<li>Image sensor
<ul>
<li>no global state</li>
<li>state travels in the requests through pipeline</li>
<li>all parameters packed into requests from application processor</li>
<li>at expose stage takes charge of lens, flash, etc. of the devices; The tags for device actions later outputs as metadata to application processor</li>
</ul>
</li>
<li>Imaging Signal processor (ISP, imaging pipeline)
<ul>
<li>receives sensor data, optionally transform some data</li>
<li>computes statistics</li>
<li>output image and statistics to application processer</li>
</ul>
</li>
<li>Visibility: full control over sensor settings; no autofocus/metering</li>
</ul>
</li>
<li>Android camera architecture
<ul>
<li>Raw Bayer Input $\rightarrow$ statisitc ($\rightarrow$ 3A); camera devices; imaging processing; raw Bayer output (with scale &amp; crop)</li>
<li>Raw Bayer Output $\rightarrow$ request control $\rightarrow$ output</li>
<li>image processing $\rightarrow$ JPEG encoder; YUV (with scale &amp; crop)$\rightarrow$ request control $\rightarrow$ output</li>
<li>image processing: hot pixel correction $\rightarrow$ demosaic $\rightarrow$ noise reduction $\rightarrow$ shading correction $\rightarrow$ geometric correction $\rightarrow$ color correction $\rightarrow$ tone curve adjustment $\rightarrow$ edge enhancement</li>
</ul>
</li>
<li>HAL v3
<ul>
<li>Application framework (camera apps) $\rightarrow$ Request queue $\rightarrow$ camera HAl implementation $\rightarrow$ frame metadata queue and image buffers</li>
<li>Camera2 API core operation model
<ul>
<li>1 request, 1 image captured; 1 reuslt metadata + N image buffers</li>
<li>Camera-using app $\rightarrow$ camera2 API capture request queue input to camera device module $\rightarrow$ camera hardware (in-flight capture queue) $\rightarrow$ output image queues and <code>onCaptureComplete()</code> back to Camera2 API $\rightarrow$ Camera-using app</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="references">References:</h2>
<ol>
<li>Malvar, H. S., He, L. W., &amp; Cutler, R. (2004, May). High-quality linear interpolation for demosaicing of Bayer-patterned color images. In 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing (Vol. 3, pp. iii-485). IEEE.</li>
</ol>

  </article>

</div>


  
<script type="text/javascript" src="/main.js" defer></script>


</body>

</html>