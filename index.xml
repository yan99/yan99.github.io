<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yang Yan&#39;s Personal Webpage on Yang Yan</title>
    <link>https://yan99.github.io/</link>
    <description>Recent content in Yang Yan&#39;s Personal Webpage on Yang Yan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://yan99.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>3A</title>
      <link>https://yan99.github.io/othernotes/3a/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/othernotes/3a/</guid>
      <description>SLR Structure With half-silvered mirror down, some light is shunted to AF, remainder reflected up to form image on diffuse focusing screen AE and viewfinder sees from the same side of the mirror. AE light meter sees focusing screen; viewfinder sees with the same perspective as main sensor With mirror rotated up, light exposes to the main sensor. Taking pictures. Viewfinders Coverages and Magnification Coverage is fraction of sensor image covered by the viewfinder: $\frac{\text{area of focusing screen}}{\text{main sensor area}}$ Magnification is apparent size of objects in viewfinder relative to unaided eye: $\frac{\theta_{\text{viewfinder angle range}}}{\theta_{\text{eye angle range}}}$ Electronic viewfinders Pros: same view as lens without reflex mirror; tone map to show effect of chosen exposure Cons: poor resolution, low dynamic range relative to optical Tilt-shift Lenses Adjust the plane of focus to make the entire scene sharp Simulate a macro lens with a shallow depth of field, hence makes any scene look like a miniature model Not a tilt-shift lens, gradient blur can &amp;ldquo;fake&amp;rdquo; the tilt-shift lens effect / Auto Focus Active or Passive: the position and directions of the illumination is known or unknown Active systems: some kind of radiation is send towards the scene (sonar, laser, structured light, etc.</description>
    </item>
    <item>
      <title>Depth Image Denoising</title>
      <link>https://yan99.github.io/projects/depthimagedenoise/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/projects/depthimagedenoise/</guid>
      <description>The github page is ImagePreparation. This implementations include three parts, the middlebury dataset together with noise models, an implementation of a CNN denoising model MSMF, and an implementation of a diffusion model DDVM (with a simple Unet, will implement with efficient Unet and the patchwise training method in the near future).&#xA;Noise Models References:&#xD;@article{Barron:etal:2013A,&#xD;author = {Jonathan T. Barron and Jitendra Malik},&#xD;title = {Intrinsic Scene Properties from a Single RGB-D Image},&#xD;journal = {CVPR},&#xD;year = {2013},&#xD;}&#xD;@article{Bohg:etal:2014,&#xD;title = {Robot arm pose estimation through pixel-wise part classification},&#xD;author = {Bohg, Jeannette and Romero, Javier and Herzog, Alexander and Schaal, Stefan},&#xD;journal = {ICRA},&#xD;year = {2014},&#xD;} MSMF, the CNN model:</description>
    </item>
    <item>
      <title>Diffusion Models for Imaging and Vision</title>
      <link>https://yan99.github.io/othernotes/diffusionmodels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/othernotes/diffusionmodels/</guid>
      <description>VAE (Variational Auto-Encoder) Setting Input $x$ (e.g. image) $\rightarrow$ Encoder $\rightarrow$ Latent Variables $z$ $\rightarrow$ Decoder $\rightarrow$ Generated $\hat{x}$ Autoencoder: an input variable $x$ and a latent variable $z$ latent space/latent feature space/embedding space: an embedding of a set of items within a manifold in which items resembling each other are positioned closer to one another &amp;ldquo;Variational&amp;rdquo;: use probabiity distriution $p(x)$ for $x$ and $p(z)$ for latent variable $z$ - $p(z|x)$: associated with the encoder.</description>
    </item>
    <item>
      <title>Notes for Computational Imaging</title>
      <link>https://yan99.github.io/othernotes/computationalimaging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/othernotes/computationalimaging/</guid>
      <description> </description>
    </item>
    <item>
      <title>Notes for Ray Tracing</title>
      <link>https://yan99.github.io/othernotes/raytracing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/othernotes/raytracing/</guid>
      <description> </description>
    </item>
    <item>
      <title>Purdue ECE 638</title>
      <link>https://yan99.github.io/notes/ece638/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/notes/ece638/</guid>
      <description>Color Science Color Basic Concepts Trichromatic theory of color: interaction of stimulus and 3 different types of receptors in eyes. Color matching experiment: half of the area display with $C_{Target}$, the other half display with $C_{match} = C_{1} + C_{2} + C_{3}$ for additive system. Let examer to match the two colors Spectral representation of color Stimulus for eyes: $S(\lambda) = R(\lambda)\cdot I(\lambda)$, where $R$ is reflectance of the surface, $I$ is the illuminant ray shoot on the surface Have to be normal, due to angular dependence Trichromatic sensor model, where $(R_{s}, G_{s}, B_{s})$ is stimulus vector, $Q$ is spectral response functions of the sensor (eyes/HVS, cameras, etc.</description>
    </item>
    <item>
      <title>Smartphone Imaging Technology and its Applications</title>
      <link>https://yan99.github.io/othernotes/smartphone_imaging_tech_review/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/othernotes/smartphone_imaging_tech_review/</guid>
      <description>Background: Evolution of the mobile phone imaging syetem for the mass consumer market From mobile phone to smartphone Smartphone today Tomorrow&amp;rsquo;s smartphones Mobile Imaging Market development Supply chain Background: Brief history and milestones of smartphone imaging technology Physical prroperties and requirements of smartphone photography Camera form factor and image sensor size Smartphone flat housing 7-10 mm thick $\rightarrow$ cell phone optic $&amp;lt; 5-6$ mm Relative flatness factor: $r = \frac{L}{\Theta_{im}}$ $L$: overall length $\Theta_{im}$: still feasible full image diagonal &amp;ldquo;The image sensor should be as large as possible so that as much light as possible falls on a pixel.</description>
    </item>
    <item>
      <title>Stanford CS 149</title>
      <link>https://yan99.github.io/notes/cs149/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/notes/cs149/</guid>
      <description>Introduction Overall Parallel thinking Decomposing work into pieces that can safely be performed in parallel Assigning work to processors Managing communication/synchronization between the processors so that it does not limit speedup Writing code Performance characteristics of implementation Design trade-offs: performance vs. convenience vs. cost Hardware Fast != efficient Fast on parallel computer does not mean that it is using the hardware efficiently Make use of provided machine capabilities (Programmer&amp;rsquo;s perspective) vs choosing the right capabilities to put in system (HW designer&amp;rsquo;s perspective) Why parallel: recent 15 years, processor performance improved on exploiting instruction-level parallelism and increasing CPU clock frequency Machine code Structure: fetch/decode $\rightarrow$ ALU (execution unit) $\rightarrow$ execution context (Registers) ALU: performs the operation Registers: maintain program state; store value of variables Instruction level parallelism (ILP) Superscalar execution: Processor automatically find independent instructions in an instruction sequence and executes them in parallel on multiple execution units Superscalar processor: decode and execute multiple instructions per clock Out-of-order control logic $\rightarrow$ fetch/decode 1 // fetch/decode 2 $\rightarrow$ execution 1 // execution 2 $\rightarrow$ execution context Ex.</description>
    </item>
    <item>
      <title>Stanford CS 231m</title>
      <link>https://yan99.github.io/notes/cs231m/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yan99.github.io/notes/cs231m/</guid>
      <description>Panoramas Can be achieved with wide-angle optics or rotation cameras.&#xA;System Overview Camera Module -&amp;gt; Video Frames -&amp;gt; Real-Time Tracking -&amp;gt; Camera Module Camera Module -&amp;gt; Images -&amp;gt; Warping -&amp;gt; Registration -&amp;gt; Blending -&amp;gt; Final Panorama Cylindrical Panoramas Project each image onto a cylinder A cylindrical image is a rectangular array View: reproject a portion of the cylinder onto a picture plane representing the display screen Narrow FOV -&amp;gt; less distortion Same center of projection.</description>
    </item>
  </channel>
</rss>
