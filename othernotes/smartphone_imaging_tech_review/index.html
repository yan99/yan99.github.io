<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
    Smartphone Imaging Technology and its Applications - Yang Yan
  </title>

  <meta name="description" content="by Vladan Blahnik and Oliver Schindelbeck" /><meta name="generator" content="Hugo 0.121.1">

  <link rel="stylesheet" href="/main.css" />

  
  
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
      integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv"
      crossorigin="anonymous"
    >

    <script defer
      src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
      integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
      crossorigin="anonymous"
    ></script>

    <script defer
      src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
      integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);">
    </script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
    </script>
  

  <meta property="og:title" content="Smartphone Imaging Technology and its Applications" />
<meta property="og:description" content="by Vladan Blahnik and Oliver Schindelbeck" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yan99.github.io/othernotes/smartphone_imaging_tech_review/" /><meta property="article:section" content="Othernotes" />





  <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Smartphone Imaging Technology and its Applications"/>
<meta name="twitter:description" content="by Vladan Blahnik and Oliver Schindelbeck"/>


  <meta itemprop="name" content="Smartphone Imaging Technology and its Applications">
<meta itemprop="description" content="by Vladan Blahnik and Oliver Schindelbeck">

<meta itemprop="wordCount" content="9138">
<meta itemprop="keywords" content="" />

  <meta itemprop="name" content="Smartphone Imaging Technology and its Applications">
<meta itemprop="description" content="by Vladan Blahnik and Oliver Schindelbeck">

<meta itemprop="wordCount" content="9138">
<meta itemprop="keywords" content="" />
</head><body class="flex relative h-full min-h-screen"><aside
  class="will-change-transform transform transition-transform -translate-x-full absolute top-0 left-0 md:relative md:translate-x-0 w-3/4 md:basis-60 h-full min-h-screen p-3 bg-slate-50 dark:bg-slate-800 border-r border-slate-200 dark:border-slate-700 flex flex-col gap-2.5 z-20 sidebar flex-shrink-0">
  <p class="font-bold mb-5 flex items-center gap-2">
    <button aria-label="Close sidebar"
      class="md:hidden menu-trigger-close p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-6 w-6" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
</svg></button>
    <a href="https://yan99.github.io/" class="px-2">
      <span>Yang Yan</span>
    </a>
    <button aria-label="Toggle dark mode"
      class="dark-mode-toggle p-2 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700"><svg class="h-4 w-4" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <circle cx="12" cy="12" r="4" />
  <path d="M3 12h1M12 3v1M20 12h1M12 20v1M5.6 5.6l.7 .7M18.4 5.6l-.7 .7M17.7 17.7l.7 .7M6.3 17.7l-.7 .7" />
</svg></button>
  </p>

  
  <ul class="list-none flex flex-col gap-1">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/" >
        <span>Home</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  text-slate-400 font-semibold pb-0 pl-1 border-b cursor-default pointer-events-none "
        href="#" >
        <span>Content</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/notes/" >
        <span>Course Notes</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/othernotes/" >
        <span>Other Notes</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm flex items-center justify-between  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="/projects/" >
        <span>Projects</span>
        
      </a>
    </li>
    
  </ul>

  <div class="flex-1"></div>

  

  <ul class="list-none flex flex-wrap justify-center gap-1 pt-2 border-t border-slate-200 dark:border-slate-600">
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">Github</span>
        
      </a>
    </li>
    
    <li>
      <a class="px-2 py-1.5 rounded-md text-sm block text-slate-800 dark:text-slate-50  hover:bg-slate-200 dark:hover:bg-slate-700 "
        href="" target="_blank" rel="noopener noreferrer">
        <span class="sr-only">LinkedIn</span>
        
        <span><svg class="h-4 w-4" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
  stroke-linecap="round" stroke-linejoin="round">
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z" />
  <rect x="2" y="9" width="4" height="12" />
  <circle cx="4" cy="4" r="2" />
</svg></span>
        
      </a>
    </li>
    
  </ul>
</aside>

<div
  class="fixed bg-slate-700 bg-opacity-5 transition duration-200 ease-in-out inset-0 z-10 pointer-events-auto md:hidden left-0 top-0 w-full h-full hidden menu-overlay">
</div>

<button aria-label="Toggle Sidebar"
  class="md:hidden absolute top-3 left-3 z-10 menu-trigger p-1 rounded text-slate-800 dark:text-slate-50 hover:bg-slate-100"><svg class="h-6 w-6" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor"
  fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" />
  <line x1="4" y1="6" x2="20" y2="6" />
  <line x1="4" y1="12" x2="20" y2="12" />
  <line x1="4" y1="18" x2="16" y2="18" />
</svg></button>





<div class="flex max-h-screen relative overflow-y-auto h-full w-full">
  
  <article class="px-6 py-20 w-full mx-auto prose lg:prose-lg dark:prose-invert h-fit prose-img:mx-auto">
    
    <a href="https://yan99.github.io/othernotes/" class="!no-underline !text-slate-500 !mb-2 !inline-block !font-normal !text-sm hover:!text-slate-700 dark:hover:!text-slate-400">‚Üê Back to list</a>

    
    <h1 class="!mb-2">Smartphone Imaging Technology and its Applications</h1>
    

    

    <h2 id="background-evolution-of-the-mobile-phone-imaging-syetem-for-the-mass-consumer-market">Background: Evolution of the mobile phone imaging syetem for the mass consumer market</h2>
<h3 id="from-mobile-phone-to-smartphone">From mobile phone to smartphone</h3>
<h3 id="smartphone-today">Smartphone today</h3>
<h3 id="tomorrows-smartphones">Tomorrow&rsquo;s smartphones</h3>
<h2 id="mobile-imaging">Mobile Imaging</h2>
<h3 id="market-development">Market development</h3>
<h3 id="supply-chain">Supply chain</h3>
<h2 id="background-brief-history-and-milestones-of-smartphone-imaging-technology">Background: Brief history and milestones of smartphone imaging technology</h2>
<h2 id="physical-prroperties-and-requirements-of-smartphone-photography">Physical prroperties and requirements of smartphone photography</h2>
<h3 id="camera-form-factor-and-image-sensor-size">Camera form factor and image sensor size</h3>
<ul>
<li>Smartphone flat housing 7-10 mm thick $\rightarrow$ cell phone optic $&lt; 5-6$ mm</li>
<li>Relative flatness factor: $r = \frac{L}{\Theta_{im}}$
<ul>
<li>$L$: overall length</li>
<li>$\Theta_{im}$: still feasible full image diagonal</li>
</ul>
</li>
<li>&ldquo;The image sensor should be as large as possible so that as much light as possible falls on a pixel. This reduces fundamental disadvantages such as image noise, a reduced by dynamic range or longer exposure times and thus motion blur.&rdquo;</li>
<li>Factor $r$ depends on the field of view of the lens and its layout (standard upright, periscopic, etc.)</li>
<li>Wide-angle lenses: $r=0.83$ (iPhone 6, an image diagonal $\Theta_{im}$ of 6 mm, an overall length $L$ of 5 mm)</li>
<li>Thicker or a slightly protruding camera housing and complex 7-lens designs: $r = 0.65$ (an image diagonal $\Theta_{im} &gt; 12$ mm)</li>
<li>Image sensor sizes of the main camera (standard wide-angle) are integrated into several high-end smartphone models.</li>
</ul>
<h3 id="image-sensor-resolution">Image sensor resolution</h3>
<ul>
<li>Standard image resolution of 12 megapixels</li>
<li>Aspect ratio (square format adjustable for both landscape and portrait format)
<ul>
<li>SPC: 4:3</li>
<li>Full-format/APS-C: 3:2</li>
</ul>
</li>
<li>Image sensor formats (&ldquo;inch values&rdquo;)
<ul>
<li>$\Theta_{im}=$ 6 mm (width 4.8 mm, height 3.6 mm) $\leftrightarrow$ 1/3</li>
<li>Inch specification was taken from old Vidicon video tubes from 1950s and corresponded to the outer glass diameter of the photoelectric front surface.</li>
<li>The sensor diagonal corresponds to about 2/3 of the inch value. (Misleading! $\rightarrow$ dimensions in mm better!)
&lt;img src=&quot;/images/Vidicon_camera_tube.png&quot; alt=&ldquo;drawing&rdquo; width=&ldquo;50%&rdquo;/&gt;</li>
</ul>
</li>
<li>Why 12 MP in 1 $\mu m$ pixel pitch era
<ul>
<li>Keeping the same image sensor size (~6 mm), the pixel pitch reduced from about 6 $\mu m$ down to 1 $\mu m$.</li>
<li>Pixel amounts far greater than 12 MP are not beneficial due to practically unused resolution.</li>
<li>Consequences of reversed pixel race: reasonable amount of image data, better image noise, and better dynamic range</li>
<li>SPC image sensors with high number of pixels are not made up of adjacent Bayer patternsm but rather &ldquo;multicell sensors&rdquo; (Tetracell by Samsung, Quad-Bayer by Sony, 4-Cell by OmniVision)
<ul>
<li>Enhance light sensitivity (but similar can be achieved by using larger pixels)</li>
<li>Flexivility: can be read out in various ways (by selecting different sensitivities/exposure time , the dynamic range can be increased, or noise can be reduced through pixel binning, or output with very hgih-resolution image)</li>
<li>Pixels available when recording and pixels used to display the images</li>
</ul>
</li>
</ul>
</li>
<li>Why 12 MP when 0.7 $\mu m$ pixel pitch by 2020
<ul>
<li>Additional cameras of different focal lengths can achieve the disired resolution of 8 or 12 MP in standard Bayer Pattern.</li>
<li>The multi-cell sensors mastering since smaller pixel pitch is challenging in mass production.</li>
<li>12 MP still good in terms of the physiological limits of the human eye. (at best no more than 2000 x 1000 pixels are required for 6.2&rsquo; display)</li>
<li>Significant more pixels are necessary for enlargement or VR &ldquo;under a magnifying glass&rdquo;</li>
</ul>
</li>
</ul>
<h3 id="optical-resolution-and-required-aperture">Optical resolution and required aperture</h3>
<ul>
<li>Definitions:
<ul>
<li>Airy dist (airy diffraction disk) : &ldquo;When light passes through any size aperture (every lens has a finite aperture), diffraction occurs. The resulting diffraction pattern, a bright region in the center, together with a series of concentric rings of decreasing intensity around it, is called the Airy disk&rdquo;</li>
<li>Point spread function (PSF): impulse response (function) of a focused optical imaging system. The Fourier transform of it is optical transfer function (OTF) of an imaging system.</li>
<li>Relative encircled energy $= 0.73$: a significant portion of the light distribution inside a square-shaped pixel.</li>
<li>Effective encircled brightness $&gt; 0.8$: the intensity transferred to a grey value distribution by the photo conversion curve and opto-electronic conversion fucntion.</li>
<li>Modulation transfer function (MTF): measure contrast as part of image quality evaluation</li>
</ul>
</li>
<li>Diameter of the airy disk of the ideal image is $d_{airy} = 2.44 \lambda \frac{f}{d_{pupil}}$, where $\lambda$ is the wavelength and $f$ is focal length, $\frac{f}{d_{pupil}}$ is f-number (e.g. f-number is 1.4, written as f/1.4)</li>
<li>An appropriate relationship between the diameter of the airy spot and the sensor pixel pitch $p$: $d_{airy} = 2 \cdot p$</li>
<li>Set the corresponding critical f-number as: $\frac{f}{d_{pupil}} = \frac{p}{1.22\cdot\lambda}$, which for 6 mm image sensor and $p = 1.2\mu m$, is $1.8$ (or written as f/1.8)</li>
<li>Telephoto lenses (normal focal lengths and short/long portrait focal lengths) with f-stops of $2.4$ or more and smaller pixels (e.g. $p = 0.8\mu m$)</li>
<li>Nyquist frequency: $f_{Nyq} = \frac{1}{2p}$
<ul>
<li>Depending on the exact position relative to the pixel grid, a vastly differing contrast can be created.$\rightarrow$ specify image quality starting at $~f_{Nyq}/2$.</li>
<li>The system constrast is often displayed simultaneously with different fine structure periods (for $p = 1.2\mu m$, $f_{Nyq}/8$ = 52 lp/mm, $f_{Nyq}/4$ = 104 lp/mm)</li>
<li>Optics MTF * Sensor MTF has an increasingly statistical character near $f_{Nyq}$ due to relative position dependence with pixel grid</li>
<li>Moir$\acute{e}$: briefringent structured filters are expensive; point spread function of SPC lens is already larger, has a low-pass effect</li>
</ul>
</li>
<li>A limited spatial frequency for ideal incoherent optical system: $\nu_{max} = \frac{1}{\lambda f_{f-number}}$, unit is lp/mm.
<ul>
<li>$MTF_{ideal}(\nu) = \frac{1}{\pi}[acrcos(\frac{\nu}{\nu_{max}}) - \frac{\nu}{\nu_{max}\sqrt{1-(\frac{\nu}{\nu_{max}})^{2}}}]$ for $\nu\leq2\nu_{max}$</li>
<li>Monotonous, almost linear for large spatial frquencies</li>
<li>The same figure for SPC also for full-frame cameras</li>
<li>SPCs are physically limited by their size alone. $\leftrightarrow$ diffraction-limited.</li>
</ul>
</li>
<li>Abberation is so small that stopping down (f stopping down) would lead to a weaker contrast
<ul>
<li>SPCs have no iris diaphragms</li>
<li>Exposure is adjusted solely via the <em>exposure time</em> and the <em>ISO</em> sensitivity via the read-out AD amplififer on image sensor.</li>
<li>For full-frame, maximum contrast is obtained at around f/4, f/5.6, or f/8 before stopping down to the diffraction limit.</li>
<li>Loss of contrast with an open aperture is due to the fact: larger aberrations are allowed for simpler, compact design.</li>
</ul>
</li>
<li>Resolution is not independent from SNR of the image sensor</li>
</ul>
<h3 id="portrait-photography-perspective-bokeh-and-depth-of-field">Portrait photography: perspective, bokeh, and depth of field</h3>
<ul>
<li>Wide-angle lens: full diagonal field of view: $tan(FOV/2) = \frac{\Theta_{im, ff}/2}{f}\rightarrow FOV = 2arctan(\Theta_{im, ff}/(2f))$
<ul>
<li>ff: full-format</li>
<li>$f$: focal length (e.g. 35 mm, 28 mm)</li>
<li>The full image diagonal for a classic standard wide-angle lens (&ldquo;allrounder lens&rdquo;) of 35 mm: $\Theta_{im, ff} = \sqrt{36^{2} + 24^{2}} = 43.3$ mm</li>
<li>$FOV \approx 75\deg$ for $f = 28$ mm</li>
<li>Equivalent SPC focal length: $f = f_{eq}\frac{\Theta_{im, SPC}}{\Theta_{im, ff}} = 3.9$ mm</li>
</ul>
</li>
<li>Close-up portrait (face almost fills the vertical image field)
<ul>
<li>The distance from entrance pupil to the object: $\frac{s}{y_{ob}} = \frac{f}{y_{im, SPC}} \rightarrow s \approx 400$ mm, where $2y_{im}$ is the total length of the side of the vertical (short direction).</li>
<li>40 cm is a typical distance at which one holds a smartphone (selfies, video calls etc.). So, an equivalent focal length of 28 mm is suitable as a front camera.</li>
<li>Typical normal portrait distances with a vertical side length of 0.72 m are about twice as large (i.e. 0.8 m). To have people completely in the picture, $y_{ob} = 2.16$ m (i.e. 2.4 m).</li>
<li>Wide-angle lenses are not well suited for portraits: $f_{eq} = 28$ mm and $s = 0.4$ m. The nose will be 10-20 % of the object distance in front of the ears, so it is imaged magnified and leads to deformations on the face.</li>
<li>Classic portrait focal lengths: equivalent focal length $f_{eq} \approx 85$ mm, which is 3 times longer.</li>
</ul>
</li>
<li>DSLR
<ul>
<li>The person is detached from background due to shallow depth of field</li>
<li>Defocused point spread function is larger with a longer focal length $\rightarrow$ spot diameter of the light source in the background with the same f-number and the same image format scales approximately in the ratio of the focal lengths.</li>
<li>SPC lenses have large depth of field.
/<img src="/images/Calculate_size_point_spread_in_depth.png" alt="drawing" width="50%"/></li>
</ul>
</li>
<li>Calculation of the size of the point spread in the depth and from the depth of field
<ul>
<li>Definitions
<ul>
<li>$s$ and $s&rsquo;$ are for out-of-focus, $s_{F}$ and $s_{F}&rsquo;$ are for focused.</li>
<li>Pupil magnification: $m_{p} = \frac{\Theta_{AP}}{\Theta_{EP}}$, typically [0.5, 1]</li>
<li>$\Theta_{AP}$: exti pupil, $\Theta_{EP}$: entrance pupil</li>
<li>$K$ is f-number (e.g. 1.8, 2.2)</li>
<li>Numerical aperture: $NA&rsquo; = \frac{1}{2K}$</li>
<li>Etendue: a property of light in an optical system, which characterizes how &ldquo;spread out&rdquo; the light in area and angle</li>
</ul>
</li>
<li>By comparing trianlges, the ratio of the radius of the defocused point image $r_{spot}$ to the position of the defocus in the image space $(s&rsquo;-s_{F}&rsquo;)$: $\frac{r_{spot}}{|s&rsquo; - s_{F}&rsquo;|} = \frac{\Theta_{AP}/2}{s_{F}&rsquo;} = \frac{1}{2K} \rightarrow r_{spot} = \frac{|s&rsquo; - s_{F}&rsquo;|}{2K}$</li>
<li>Focusing conditions: $\frac{1}{f_{ob}} + \frac{1}{f_{im}} = \frac{1}{f} \rightarrow -\frac{1}{m_{p}s} + \frac{m_{p}}{s&rsquo;} = \frac{1}{f}$ and $-\frac{1}{m_{p}s_{F}} + \frac{m_{p}}{s_{F}&rsquo;} = \frac{1}{f}$</li>
<li>By substitution, $r_{spot} = frac{f^{2}}{2K}\frac{|s - s_{F}|}{(f/m_{p} + s)(f/m_{p}+ s_{F})}$</li>
<li>With $f/m_{f} \approx 0$, the diameter of the image&rsquo;s <strong>circle of confusion</strong>: $\Theta_{spot} = 2r_{spot} = \frac{f^{2}}{K}\frac{|s_{F}-s|}{s_{F}s}$</li>
<li>For easier comparing the imaging lenses with different image formats, <strong>relative circle of confusion</strong>: $\Theta_{rel.spot} = \frac{\Theta_{spot}}{\Theta_{im}}$, directly indicates the spot size ion the defocused area as it appears in the photo</li>
<li>For a infinitely distant background, $\Theta_{rel.spot, \inf} = \lim_{s\rightarrow\inf}\frac{f^{2}}{\Theta_{im}Ks_{F}}$</li>
<li>With definition of f-number: $K = \frac{f}{\Theta_{EP}}$ and magnification $m = \frac{\Theta_{im}}{\Theta_{ob, Portrait}} \approx \frac{f}{s_{F}}$, $\Theta_{rel.spot, \inf} = \frac{\Theta_{EP}}{\Theta_{im}}m$</li>
<li>For approximate object distance 700 mm: $\Theta_{rel.spot, \inf} = \frac{\Theta_{EP}}{700 \text{ mm}}$</li>
<li>$\Theta_{EP, ff} = \frac{f}{K} = \frac{135\text{ mm}}{2} = 67.5 \text{ mm}$, $\Theta_{rel, spot, ff} \approx 10%$; $\Theta_{EP, SPC} = \frac{f}{K} = \frac{4\text{ mm}}{2} = 2.3 \text{ mm}$, $\Theta_{rel, spot, SPC} \approx 0.3%$</li>
<li>With $\tan(FOV/2) = \frac{\Theta_{im}/2}{f}$, $\Theta_{rel.spot, \inf} = \frac{\Theta_{im}NA&rsquo;}{700\text{ mm}\cdot\tan(FOV/2)}$. With a given FOV, the <strong>relative diameter of the circle of confusion</strong> is directly proportional to the product of <strong>image field diameter</strong> and the <strong>numerical aperture</strong> of the image.</li>
<li>Optical systems <strong>Etendue</strong> (&ldquo;throughput&rdquo;/&ldquo;collecting power&rdquo;/&ldquo;A$\Omega$ product&rdquo;): $G = \frac{\pi}{4}\Theta_{im}^{2}NA&rsquo;^{2} \rightarrow $\Theta_{rel.spot, \inf} = \frac{\sqrt{G}}{700\text{ mm}\cdot \tan{FOV/2}}\rightarrow \Theta_{rel.spot, \inf}\propto\sqrt{G}\rightarrow$ 7x smaller image diameter and 7x larger NA&rsquo; achieve the same background circle of confusion. But not feasible for high-aperture lenses (SPC lens)</li>
<li>Depth-of-field: threshold the size of the circle of confusion $\Theta_{thres} = \Theta_{im}/1500$.</li>
<li>Hyperfocal distance: $s_{F, hyp} = \frac{f^{2}}{K\Theta_{thres}} + f \rightarrow \frac{f}{2K\tan{FOV/2}/1500} + f$. The image is shape from $s_{F, hyp}/2$ to infinity.</li>
<li>Autofocus is only required for close range.</li>
<li>SPC &ldquo;portrait mode&rdquo; achieved with depth and/or high-resolution 3D sensors</li>
</ul>
</li>
</ul>
<h3 id="acutee-tendue-and-photographic-exposure">$\acute{E}$ tendue and photographic exposure</h3>
<ul>
<li>Exposure controlled by ISO (sensitivity of the image sensor), exposure time, and amount of light described by $\acute{E}$tendue $H\propto ISO\times (\frac{1}{K})^{2}\times T$</li>
<li>SPC does not have iris diaphragms for controlling aperture size</li>
<li>ISO
<ul>
<li>Sensitivity of the image sensor $\rightarrow$ depends on the area of a pixel (pixel pitch size) $\leftarrow$ No. of photons strike each time</li>
<li>Efficency for a pixel absorbs light and converts into an electrical voltage</li>
<li>No. of photon per pixel is inversely proportional to $c^{2}$, where $c$ is <strong>crop factor</strong>. Crop factor (format factor/focal length multiplier) $c$ of an image sensor format is the ratio of the dimensions of a camera&rsquo;s imaging area compared to a  reference formrat, usually 35 mm format (full-frame)</li>
<li>$c = 7\rightarrow$ 49 times fewer photons. This corresponds to 5-6 exposure values ($EV = \log_{2}(K^{2}/T)$). $\rightarrow$ issues in low light and/or with fast-moving objects</li>
<li>High level of sensitivity $\rightarrow$ short exposure time and increased image noise</li>
</ul>
</li>
<li>Sunny 16 rule
<ul>
<li>On a sunny day with an ISO 100 film and f/16, the required exposure time is about 0.01 second</li>
<li>E.g. increase 3 f-stops in aperture to f/5.6 $\rightarrow$ 8 times shorter exposure (1/800 second) or ISO of 50 and 1/400 s</li>
</ul>
</li>
<li>Long exposure times cause issues in shaking hand in still photographs $\rightarrow$ reduced in SPC through <strong>optical and electronical image stabilization</strong></li>
<li>Image sensor technology (&ldquo;binning&rdquo;, &ldquo;deep trench isolation&rdquo;, &ldquo;dual conversion gain&rdquo;, &ldquo;higher quantum efficiency&rdquo;)
<ul>
<li>Binning: squares of pixels are binned into larger pixels. Artificially increase pixel size, gather more signal, flexibility of a camera sensor
<ul>
<li>CCD/EMCCD sensor is binned on the sensor <strong>before readout</strong>, meaning that it occurs before read noise is introduced by converting photoeletrons into grey levels. improving SNR and increased frame rate.</li>
<li>CMOS sensor is binned off the sensor <strong>after readout</strong>, meanning that read noise has been introduced to each pixel. Combining a 2x2 section of pixels together results in <strong>double</strong> the read noise for resulting super-pixel. <strong>4 times signal 2 times read noise</strong> $\rightarrow 2:1$ boost in SNR, no speed benefits. However, CMOS camera is already far faster than CCD/EMCCD.</li>
<li>CMOS Binning. 3 noise sources: photon shot noise ($\sqrt{s}$), read noise (a fixed value for the sensor), and dark current; Noise are <strong>adding in quadrature</strong> ($\sqrt{\sum{n^{2}}}$)</li>
</ul>
</li>
<li>Deep trench isolation: suppress electrical crosstalk; a DTI sidewalls passivation step is needed to avoid any degradation on dark current and white pixel number due to additional interface degects caused by DTI etch</li>
<li>Dual conversion gain licensed by Sony: 2 readout modes: one that includes a capacitor in the path, to provide extra electron storage capacity for bright, high DR conditions; the other that disengages this capacitor, delivers less dynamic range but increases the conversion gain, boosting the signal for low light conditions; before readout noise introduced</li>
<li>Higher quantum efficiency (QE): measure of effectiveness of an imaging device to convert incident photons into electrons (95% means sensor exposed to 100 photons producing 95 electrons of signal, from 95% to 20% varies by photons wavelengths)
<ul>
<li>Blue response is reduced due to front surface recombination. Front surface passivation (fabrication prrocess) affects carriers generated near the surface, and since blue light is absorbed very close to the surface.</li>
<li>Green response is reduced causing by reflection and a low diffusion length. Green light is absorbed in the bulk of a solar cell and a low diffusion length will affect the collection probability from the solar cell bulk and then reduce the QE in the green protion.</li>
<li>Red response is reduced due to rear surface recombination, reduced absorption at long wavelengths and low diffusion lengths.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="david-versus-goliath-the-pros-and-cons-of-miniaturization">David versus Goliath: the pros and cons of miniaturization</h3>
<ul>
<li>Crop factor
<ul>
<li>$c = \frac{\Theta_{im, ff}}{\Theta_{im, SPC}} = \frac{43.3\text{ mm}}{\Theta_{im, SPC}}$, $c = 3.5$ to $12$</li>
<li>Depends on SPC sensor size, and lens FOV</li>
<li>With the assumption of containing equal number of pixels (12 MP), pixel pitch is larger by $c$ for full-format sensor</li>
<li>Further assumptions of same f-number $K$ and same FOV, $\bar{f} = f/c$, $\bar{\Theta_{im}} = \Theta_{im}/c$, $\bar{L} = L/c$ are scaled inversely with crop factor; Entendue/throughput and sensor area are scaled by $/c^{2}$.</li>
<li>Advantages
<ul>
<li>Geometrical scaling including weight and volume reduction</li>
<li>Optical distance (can focus on much smaller subjects)</li>
<li>depth-of-field scales linearly with focal length $f$ meaning that the hyperfocal distance $s_{F, hyp}$ is a factor of $c$ further away from the miniature lens. This increases image quality (less focusing errors). However, not capable for creating an artistic shallow depth of field.</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>sees much less ligiht ($1/c^{2}$ since pixel area reduction, either $c^{2}$ larger exposure time or ISO, large ISO introducing more noise by $c$; $H~ISO\times (1/K^{2})\times T$)</li>
<li>Not capable for shallow depth of field</li>
</ul>
</li>
</ul>
</li>
<li>Lens aberrations
<ul>
<li><strong>Ray aberration spot</strong> scaled down by $1/c$ and <strong>pixel pitch</strong> scaled down by $1/c$</li>
<li><strong>Airy spot size</strong> (diameter of airy diffraction disk, $\Theta_{airy} = 2.44\lambda K$; diffration point spread function) stays the same</li>
<li>Though ray aberration spot stays the same relative to the scale of pixel, the <strong>resolution</strong> is dropped linearly by $1/c$ due to the diffraction-limited lens design.</li>
<li>Lohmann&rsquo;s scaling law, the spot area $A_{p}&rsquo; = \lambda^{2} K^{2} + (1/c)^{2}\bar{\epsilon^{2}}$, where $\bar{\epsilon^{2}}$ is the second moment of the ray deviation distribution.</li>
</ul>
</li>
<li>Multi-camera systems containing many thin lenses in parallel.
<ul>
<li>Increase effective image sensor area</li>
<li>Combining the images of various cameras to improve the image performance in various directions (e.g. noise reduction, HDR)</li>
<li>Stereographic 3D depth</li>
</ul>
</li>
</ul>
<h3 id="spc-lenses-quality-evaluation">SPC lenses: quality evaluation</h3>
<ul>
<li>A common choice of spatial frequencies for MTF evaluations at full format is 10, 20, and 40 lp/mm, which are corresponding to $Nyq/8$, $Nyq/4$, $Nyq/2$ for full-frame format camera pixel pitch size $p = 6.25\text{ } \mu m$, respectively ($Nyq = 1000/2p$ lp/mm, $p$ in $\mu m$)</li>
<li>MTF at original size (ff/8) is only slightly maller compared to the upscaled version of the lens $\rightarrow$ upscaled SPC to full-frame is comparable with excellent full format lenses</li>
<li>According to the Lohmann&rsquo;s scaling law, at t he original size with respect to imaging with smaller pixel pitch is moly moderately limited by diffraction compared to the aberration level of the lens design.</li>
<li>When the lens size is scaled down the overall performance severely drops. The diffraction contribution in Lohmann&rsquo;s scaling law becomes dominant.
<ul>
<li>Strehl ratio ([0,1]): evaluate lens performance in comparison to the diffraction limit. Strehl ratio $\uparrow$, diffraction limited $\uparrow$</li>
<li>Performance is pirncipally limited for yet smaller lens sizes</li>
<li>Predominantly diffrection-limited</li>
</ul>
</li>
<li>Makes little sense to achieve pixel resolutions far below $0.7-0.8\mu m$, which is the current state of the art for image sensor CMOS technology</li>
</ul>
<h2 id="the-multicamera-system-in-modern-smartphones">The multicamera system in modern smartphones</h2>
<ul>
<li>Standard wide-angle camera as main camera: 28 mm, FOV 75$\deg$</li>
<li>Multicamera system for rear: a main camera, a shorter focal length with FOV of 120$\deg$, a longer focal length (55 mm or 70 m or 125 mm), a 3D depth sensor (based on ToF measurement) for a real-time depth maps of a scene</li>
<li>Multicamera system for front: standard wide-angle lens + larger camera with a high-resolution image sensor (multicell for HDR) and autofocus, 3D sensing camera for face recognition (next to the visual cameras)</li>
<li>Diagonal field of view, equivalent focal length, sensor size, sensor pixel number, pixel pitch, f-number, focal length, MOD (minimum optical distance), depth of field, image stabilisation</li>
</ul>
<h2 id="optical-system-design">Optical system design</h2>
<h3 id="optical-design-structure-of-a-smartphone">Optical design structure of a smartphone</h3>
<ul>
<li>Criteria
<ul>
<li>Substracting the housing and image sensor thickness, the overall length of the lens must not be longer than about 5-6 mm</li>
<li>The image sensor should be as large as possible for reducing the disadvantages of a small image sensors (noise, dynamic range, see less photon)</li>
<li>The aperture ofoo the lens must be relatively large, about f/2 or larger, so that the system resolution is not limited with image pixel sizes</li>
</ul>
</li>
<li>Relative flatness factor $r = \frac{L}{\Theta_{im}} \approx 0.65 \cdots 0.85$; Despite the highly miniaturized desing, the crop factor is around $4$ away frorm ff image sensors</li>
<li>Why plastic asperical lens for SPCs
<ul>
<li>No sperical lens type has large aperture of around f/2 and a small length-to-image-diameter ratio at the same time.</li>
<li>Asperical lens for SPCs are shorter in length and better in performance:
<ul>
<li>Higher contrast and lower peripheral light intensity drop (both due to lack of vignetting)</li>
<li>Both distortion and chromatic aberrations are comparably very good</li>
</ul>
</li>
</ul>
</li>
<li>Plastic spherical lens$\rightarrow$ Plastic aspherical lens; FOVs around $60\deg$ and $75\deg\rightarrow$ $20\deg-150\deg$</li>
<li>As pixel shrinking: Doublets/triplets$\rightarrow$ 4 lenses$\rightarrow$ 7 lenses and more; f-number decreased</li>
<li>The optical design of SPC lenses is rarely dealt with in the literature</li>
<li>Comparisons between Biogon lens and SPC lens
<ul>
<li>Biogon lens
<ul>
<li>Negative outer lens + positive inner lens group</li>
<li>Negtive outer element: chiefray is bent to a smaller angle inside the lens $\rightarrow$ smaller aberration contributions; off-axis entrance pupil becomes larger, improving illumination</li>
<li><strong>Asymmetrical aberrations</strong> including <strong>distortion, coma, and lateral chromatic</strong> are elimiated by the quasi-symmetric arrangement around the stop in the center of the system. They get cancelled.
&lt;img src=&quot;/images/chromatic_aberration.png&quot; alt=&ldquo;drawing&rdquo; width=&ldquo;50%&rdquo;/&gt;</li>
<li><strong>Longitudinal</strong> and <strong>high-order chromatic aberrations</strong> are corrected by low-dispersion glasses for the outer negative lens and the achromats of the inner positive lens</li>
<li><strong>Spherical aberration</strong> and <strong>astigmatism</strong> are remainning to be corrected by fine-tunning all lens parameters.</li>
<li>Astigmatism: one where rays that propagate in two perpendicularr planes have different foci.</li>
</ul>
</li>
<li>SPC lens
<ul>
<li>Both have field of view about $80\deg$</li>
<li>Ray angle at the entrance of the lens $\approx$ at the lens exit</li>
<li>Half of the system structure to the image plane is sufficient: aberrations (in particular distortion and coma) are corrected by the <strong>aspheres</strong>.</li>
<li>With the strong aspherical design, the digital correction would bring almost <strong>no advantage</strong>.</li>
</ul>
</li>
</ul>
</li>
<li>Wide-angle lenses for SLR cameras are forced to use a retrofocus type becuase of the space required for folding mirrow between the last lens and the image sensor</li>
<li>Modern lenses for mirrorless cameras are increasingly asymmetrical.</li>
<li>Correction distortion, curvature, and astigmatism $\rightarrow$ aspherical lenses are placed directly in front of the image plane</li>
<li>SPC lens
<ul>
<li>High-order awberrations are used here to reduce low-order aberrations</li>
<li>All lens surfaces are aspherical</li>
<li>Standard surface description of aspheres: $z = \frac{xrr^{2}}{1 + \sqrt{1-(1+k)c^{2}r^{2}}} + a_{4}R^{4} + a_{6}r^{6}+ \cdots$, where $c$ denotes the curvature at the apex of the surface, $k$ denotes the conical constant and $r$ denotes the radial distance from the optical axis.
<ul>
<li>The first term: different conic shapes: $k = 0$, sphere; $-1&lt;k&lt;0$, ellipsoid with main axis on the optical axis; $k = -1$, paraboloid; $k &lt; -1$, hyperboloid</li>
<li>Typical asphere for SPC: w-shaped lens</li>
<li>The low aberration orders are largely compensated for with high-order aspheres, but residual high-order aberrations remain</li>
</ul>
</li>
<li>The pupil and the field must be sampled sufficiently</li>
<li>SPC optics must <strong>hardly</strong> have any <strong>vignetting</strong> by lens edges or aprtures $\rightarrow$ reducing the aperture and loss of resolution towards the image corners</li>
<li>Mobile wide-angle lenses
<ul>
<li>The geometric light path to the corner of the image is more than 20% langer than it is to the center of the image</li>
<li>The numerical aperture size remains the same up to the edge of the image $\rightarrow$ keep the diffraction-limited resolution almost constant up to the corner of the image $\rightarrow$ change the refractive power depending on the image field height $\rightarrow$ w-shaped last lens (refractive power at edge: positive, center: negative; varies over the field height) $\rightarrow$ aperture at the field edge &raquo; conventional spherical optic</li>
<li>Characteristic of standard wide-angle lenses: chief rays strike the image plane at a simiarly high angle as they enter the lens ($\pm 35\deg-40\deg$)</li>
<li>For standard wide-angle lense, the chief ray runs along a line through the lens; for telephoto and extreme wide-angle types, chief ray has a global bending $\rightarrow$ standard wide-angle lenses are the most compact lens type among SPC lenses; image sensor used for standard wide-angle lens iat he largest within the multicamera systme</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="optical-design-imaging-performance">Optical design imaging performance</h3>
<ul>
<li>Optical image performance evaluation of camera lens design: <strong>MTF</strong> (vs. FOV, spatial frequencies, distance to the image plane), <strong>distortion</strong> (radial distortion, TV distortion, and distortion grid plot), <strong>relative illumination</strong>, <strong>aberrration</strong> (spot diagrams, ray aberration curves, chromatic focus and lateral shift), <strong>angle of incidence at image sensor</strong></li>
<li>MTF vs. spacial frequencies up to the cut-off frequency ($1/(\lambda K)$)
<ul>
<li>Image performance of all lenses if diffraction-limited near the image center and drops off</li>
<li>The through-focus region with very high contrast is roughly only about $\pm 10 \mu m$ for all lenses</li>
</ul>
</li>
<li>Distortion graph
<ul>
<li>Not noticeable (&lt;1%)</li>
<li>Barreldistortion of the extreme wide-angle lens is about 20%, which is not corrected by software as well $\rightarrow$ perspective distortion</li>
</ul>
</li>
<li>Relative illumination
<ul>
<li>Drops more towards toe corner of the image field for wide-angle lenses (&ldquo;shading&rdquo;). This is corrected by software and cause an in crease noise sensitivity by 1 or 2 EV ofr wide-angle photography</li>
<li>Includes a natural geometrical loss according to approxinmately $\cos^{4}{angle-of-incidence}=\cos^{4}{35\deg}\approx 0.45$</li>
</ul>
</li>
<li>Angle-of-incidence graph shows the chief ray and marginal ray angles in the image plane
<ul>
<li>Avoiding light loss, the chief ray incidence angle is limited to $&lt;35\def$</li>
<li>The <strong>oblique incidence</strong> on the image sensor results in further intensity losses (included in the software-corrected shading) $\leftarrow$ improved by <strong>back-side-illuminated</strong> image sensors, <strong>specific architexture</strong> (e.g. deep-trench structures), and <strong>slightly shifting the micro lenses</strong> according to the incidence angle</li>
</ul>
</li>
<li>Ray aberration on the tangential and sagittal image plane for different wavelengths versus field
<ul>
<li>Averration curves are very &ldquo;wiggly&rdquo; as <strong>residual aberrations</strong> of the compensation principle of low-order aberrations by high-order aberrations through usage of high-order aspherical lens surface deformations</li>
<li>Chromatic aberrations can be physically evaluated (image simulations of edge or line spread functions vs. field and through focus; &ldquo;color fringe widths&rdquo;)</li>
</ul>
</li>
</ul>
<h3 id="extreme-wide-angle-lenses">Extreme wide-angle lenses</h3>
<ul>
<li>Short flatness factor $r = L/\Theta_{im}&lt;1$; with additional negative &ldquo;bending lens&rdquo; at the front, $r$ is even larger</li>
<li>Beam path is comparable with standard wide-angle lens; chief ray angles $\approx 35 \deg$ on the image plane</li>
<li>Optical desings of extreme wide-angle lenses with an even larger FOV of up to around $150\deg$ and $160\deg$ with ~50% distortion</li>
</ul>
<h3 id="tele-lenses">Tele lenses</h3>
<ul>
<li>Smaller image sensors and smaller apertures</li>
<li>Difficulties of compact telephoto lenses: longer $f$, samller the required telefactor($TF = L/f$)
<ul>
<li>TF&lt;1: refractive power is positive in the front part and lens and negative in the rrear; The smaller TF more positive orr negative refractive power required, then leads to greater lens curvaturres and larger aberrations</li>
<li>Optical performance is severely limited as the $f\uparrow$</li>
</ul>
</li>
</ul>
<h3 id="periscope-tele-lenses-and-alternative-tele-concepts">Periscope tele lenses and alternative tele concepts</h3>
<ul>
<li>Periscope layout: with $45\deg$ mirror or with $45\deg$ prism mirror
<ul>
<li>Mirror size and entrance pupil size &lt; depth of the housing</li>
<li>$f\uparrow\rightarrow K\uparrow\rightarrow$ the diffraction limited resolution weaker</li>
<li>e.g. $\Theta_{EP} = 4$ mm, relative small aperture of $f/3.4$, $f = K\cdot\Theta_{EP}=13.2$ mm</li>
</ul>
</li>
<li>Entrance pupil can be made rectangular with a larger f-number in the long direction</li>
<li>Catadioptric layout: 2 mirrors in the front or with several reflections between the mirrors ($TF&lt;0.5$)
<ul>
<li>Allows a very large entrance pupil diameter $\rightarrow$ high aperture ratio</li>
<li>Aperture ratios can reach to f/1 with large cetral obscuration $\rightarrow$ distinct loss of contrast in the lower spatial frequencies $\rightarrow$ fine structures would be displayed with a higher contrast (irrelevant to SP dimensions, this high resolution cannot be used due to the available pixel sizes)</li>
<li>Pors: loss of contrast; &ldquo;donut bokeh&rdquo; bused by obscuration (i.e. ring-shaped out-of-focus highlights)</li>
</ul>
</li>
</ul>
<h2 id="zoom">Zoom</h2>
<ul>
<li>Very good optical zoom systems for compact digital system cameras (DSC) also integrated in mobile phone</li>
<li>Digital zoom was also available very early on cell phones</li>
<li>Since 2016, standard: hybrid zooms through multi-camera systems using lenses of different focal lengths</li>
</ul>
<h3 id="hybrid-zoom-in-multicamera-systems">Hybrid zoom in multicamera systems</h3>
<ul>
<li>The combination of different fixed focal lengths in modern SP multicamera systems</li>
<li>The achievable optical resolution of SPC lenses is heavily dependent on the specific optical design, in turns depends on an equivalent $f$.
<ul>
<li>Assuming a diffraction-limited optical resolution of $res_optics = 0.5\cdot \Theta_{airy} = 1.22\cdot\lambda\cdot K$</li>
<li>All these resolved area on the image sensor surface (4:3 aspect ratio) $A_{im} = \pi\cdot(\Theta_{im}/2)^{2}\cdot 0.48 = \pi/4\cdot0.48\cdot\Theta_{im}^{2}$</li>
<li>The number of &ldquo;optical pixels&rdquo; (i.e. resolved areas, optical resolution ) $NP_{optics} = \frac{A_{im}}{\frac{\pi}{4}res_{optics}^{2}}\approx 0.32\frac{\Theta_{im}^{2}}{\lambda^{2}K^{2}}$</li>
</ul>
</li>
<li>Simplified consideration for digital zoom and the image sensorr resolution to estimate the total resolution of a <strong>hybrid multi-camera zoom system</strong> over the entire focal length range: $NP_{\text{optics digital zoomed}} = NP_{potics, 0}\frac{f_{0}^{2}}{f^{2}}$, where index 0 corresponding to lens index 0
<ul>
<li>Digital zoom reduces the resolution according to the cropped FOV, whcih is directly scales with $f$</li>
<li>For image sensor that has 4-pixel of a Quad-Bayer sensor structure $NP_{sensor} = \frac{NP_{sensor, total}}{NP_{macro-pixel}}$, $NP_{macro-pixel} = 1$ for standard Bayer sensor and $NP_{macro-pixel} = 4\text{ or }{9}$ for 2x2 or 3x3 multi-cell sensor.</li>
</ul>
</li>
<li>A well-balanced system should have about the <strong>same</strong> resolution of <strong>optics</strong> and <strong>image sensor</strong>
<ul>
<li>Roughly the case for <strong>extreme wide-angle</strong> and <strong>standard tele camera lenses</strong></li>
<li>For main camera, the <strong>optical resolution</strong> is clearly bettern than the image sensor resolution. Sensor limits the overall resolution</li>
<li>For periscopic long tele camera, the image sensor limits the overall resolution</li>
<li>Optical performance of wide-angle lens &gt; tele and periscope tele lenses even at the same effective focal length when zoomed in digitally</li>
<li>The actual resolution of the complete wide-angle camera system &lt; tele cameras, since the digitally zoomed-in pixel resolution is worse for $f_{eq}&gt;56$ mm</li>
<li>$NP_{effective} = min(NP_{optics digital zoomed}, NP_{sensor})$</li>
</ul>
</li>
<li>Improve image performance with image fusion in the common FOV of both cameras
<ul>
<li>Simulate of all steps in camera ISP and the specific algorithm how images are fused by multiple cameras</li>
<li>Requires joined camera module calibration</li>
<li>Hard at close range due to the parallax of the images caused by the spacing of the camera modules</li>
</ul>
</li>
</ul>
<h3 id="optical-zoom-systems">Optical zoom systems</h3>
<ul>
<li>A classical optical zoom changes the imaged object frame (i.e. FOV) by changing $f$ of the system $\rightarrow$ changing the distance between the lens between the lens elements</li>
<li>Two optical groups with $f_{1}$ and $f_{2}$: $\frac{1}{f} = \frac{1}{f_{1}} + \frac{1}{f_{1}} - \frac{d}{f_{1}f_{2}}$</li>
<li>For SP, the realized image performance of an optical zoom system can decrease significantly <strong>at long focal lengths</strong>; The image sharpness drops due to the diffraction limitation</li>
<li>For DSC, excellent image performance can be achieved with relatively loose space constraints
<ul>
<li>Digital-optical co-optimization</li>
<li>A significant portion of lens elements are aspherical</li>
<li>Digital aberration corrections are made (wid-angle range, a distortion of approx 20-30%) $\rightarrow$ image is cropped somewhat depending on the zoom</li>
<li>Zooms with a large zoom factor, the aperture in the long focal length area is reduced to limit the diameter in the front area and the overall length</li>
<li>Can reach to 20x or 50x which SPCs unachievable</li>
</ul>
</li>
</ul>
<h2 id="opto-mechanical-layout-and-manufacturing">Opto-mechanical layout and manufacturing</h2>
<h3 id="plastic-lenses-key-miniature-opto-mechanical-layout">Plastic lenses: Key miniature opto-mechanical layout</h3>
<ul>
<li>Pros:
<ul>
<li>Distinctly aspherical lens shape</li>
<li>Key to the small depth of SPC optics</li>
<li>High complexity of the structural shapes</li>
<li>Possible to implemnt and mechanical mount</li>
<li>Accuracy in the sub-$\mu m$ ranges</li>
</ul>
</li>
<li>Special noncontact interferometric measurement technology for measuring small, complex components at steep angles of incidence</li>
<li>Contact-mode measurement devices for measure component shapes</li>
</ul>
<h3 id="opto-mechanical-layout">Opto-mechanical layout</h3>
<ul>
<li>No measurements during assumbly process. MTF measurements are carried out once assembly is complete</li>
<li>Improve quality, individual lens elements are matched to another from the injection molding cavities.</li>
<li>The lens elements are positioned directly on top of each other on the flat plastic mountint rings (the ring stops to prevent straylight)</li>
<li>SPC lenses: high accuracy, sensitive to decentering or tilting</li>
<li>Plastic lenses manufacturing by injection/pression into precise molds while still in liquid form; not stable as glass; manufactured after about 1 minute</li>
<li>Plastic lenses cons: relatively low refractive indices with a large dispersion $\rightarrow$ mitigated in the future with nano composites</li>
<li>Limitations
<ul>
<li>No cemented lense like  for glass lenses</li>
<li>Thermal sensitivity $\rightarrow$ refractive index and expansion coeffcient $\rightarrow$ aberration type: focus shift $\rightarrow$ compensated by autofocus</li>
</ul>
</li>
</ul>
<h3 id="active-optical-assembly">Active optical assembly</h3>
<ul>
<li>The lenses are aligned with the image sensor and glued in with UV adhesive; Automatically in a few second</li>
<li>During the assembly process on the sensor, the barrel is aligned in the degrees of freedom
<ul>
<li>centering x/y, tilt x/y and focus distance until the specified spatial frequency response (SFR) values are reached simultaneously over the image field</li>
<li>The MTF values is monitored over long periods of time</li>
</ul>
</li>
<li>In 6 axes, the optical axis between camera modules</li>
</ul>
<h3 id="tolerancing-and-yield-analysis">Tolerancing and yield analysis</h3>
<ul>
<li>Set permitted deviations from the theorertically achievable image performance</li>
<li>Inspection process:
<ul>
<li>During the production processm, the quality of the optics is qualified on the basis of MTF values during the final inspection done by the optics module supplier for the system integrator. (not yet connected to image sensor)</li>
<li>The system integrrator adjusts the optics to the image sensor so that the final image performance of the SPC qualified with SFR measurements.</li>
<li>SFR is the common notation of the MTF of the complete system optics/sensor</li>
</ul>
</li>
<li>Yield analyses (reject analyses)
<ul>
<li>Done in the final stages of optical design with Monte Carlo analyses</li>
<li>The optical desingers use their own sensitivity analyses to set tolerances for the lens (radius, thickness, aspherical deviations, deviations in the regractive index, dispersion of the plastic) and their relative positioning errors in assembly (decentering, tilting, spacing deviations, etc.) and probability distributions of individual errors.</li>
<li>&ldquo;Rolls&rdquo; many different systems</li>
<li>MTF data for these systems gives a statistical distribution $\rightarrow$ system MTF specification</li>
</ul>
</li>
</ul>
<h3 id="wafer-level-manufacturing">Wafer-level Manufacturing</h3>
<ul>
<li>Low-cost; thin</li>
<li>Micro-electromechanical systems (MEMS)-based sensors (e.g. gyroscope, accelerrometer), photonic chips (photonic integrated circuits (PICs))</li>
</ul>
<h3 id="anti-reflection-coating-for-plastic-lenses">Anti-reflection coating for plastic lenses</h3>
<ul>
<li>Important in scenes with a large dynamic range, especially with bright light sources (residual light reflections straylight or &ldquo;ghosts&rdquo; on the image plane)</li>
<li>Reflections on optical surfaces $\leftarrow$ destructive interference from a single or multilayer coating by properly choosing layer thicknesses and material refractive index
<ul>
<li>Camera lenses have AR coatings with multilayer coating of 2 or 3 materials of thicknesses of about 10 to a few 100 nm</li>
<li>Coating process cannot easily transferred to glass (adhesion, lower melting temp, etc.)</li>
<li>A uniform coating thickness is difficult $\rightarrow$ atomic layer deposition (ALD)</li>
</ul>
</li>
</ul>
<h2 id="image-sensor">Image sensor</h2>
<ul>
<li>A CMOS sensor is a matrix of semiconductor photodiodes that detects the irradiance distribution on the sensor surface.
<ul>
<li>According to the irradiance distribution on the sensor chip and the exposure time ($T$), electrons are generated as charge carriers in the individual photodiodes and converted by capacitors, a voltage is finally generated. Then, A to D.</li>
<li>Each individual photodiode is provided with its own electronic circuit: <strong>readout amplifier</strong> read out individually at each XY coordinates</li>
<li>A higher bit value in AD converter does not necessarily lead to better image quality; image quality depends on <strong>noise behavior</strong> of the image sensor and <strong>image motif</strong></li>
<li>Only portion of the cell surface is sensitive to light due to wiring</li>
<li>Microlens covers the the entire sensor for collecting as much of the incident light as possible and <strong>avoiding shadowing</strong> within the cell structure</li>
<li>The voltage level ($\rightarrow$ image signal) on the individual photodiodes depend <strong>solely</strong> on the respective <strong>brightness</strong> and the <strong>exposure time ($T$)</strong></li>
<li>Color filter array (Bayer pattern, RGGB) $\rightarrow$ de-Bayering/demosaicing; The color cameras with a Bayer mask have a lower resolution thanmonochrome cameras $\rightarrow$ some SPC multicamera systems equiped with monochrome camera modules for high-resolution image and color information from second camera</li>
<li>DSLR with mechanical shutter $\rightarrow$ almost simultaneous exposure; SPC with purely electronically exposure control, exposure is done during <strong>readout</strong> $\rightarrow$ individual photodiodes are not exposed and readout at the same time $\rightarrow$ runtime effects i.e. <strong>rolling shutters</strong> limiting in <strong>slow motion mode</strong>; electrronic &ldquo;global shutter&rdquo; (Sony 2017)</li>
<li>Additional IR band pass filter to block residual significant portion of light in IR (silicon has a monotonically increasing sensitivity from blue towards IR); For 3D face or iris recognition, IR cut-off at e.g. 840 nm or 950 nm.</li>
</ul>
</li>
<li>OECF (Opto-Electronic Converrsion Function) response/characteristic curve
<ul>
<li>Horizontal axis: physical intensity of the light received by the image sensor; Irradiance (radiation power per area, $W/m^{2}$) or Luminous efficiency function (CIE standard, luminance intensity per area, $cd/m^{2}$); in linear scale or log scale</li>
<li>Vertical axis: digital numeric value (0-255 for 8 bits, 0 to 1000 for 10 bits)</li>
</ul>
</li>
<li>Photoconversion curve/camera response function
<ul>
<li>Horizontal axis: exposure value (EV, $EV = \log_{2}(k^{2}/T)$)</li>
<li>Vertical axis: no. of electrons ($\equiv$ digital numeric value)</li>
<li>Relation between the number of electrons and the luminance</li>
<li><strong>Noise limit</strong>: minimum brightness on the pixel (minimum signal); signal rises from the noise limit; forr SPC CMOS sensors: 1-2 electrons</li>
<li><strong>FWC</strong> (full well capacity): the capacity of the potential well of the diode; CMOS sensors: 4k - 5k electrons</li>
<li>0 EV set at FWC or close to the saturation value $\rightarrow$ noise limit/value can be readout</li>
<li>Dynamic range: ratio of the maximum and minimum signal (e.g. here is $10$ EV)</li>
</ul>
</li>
<li>Quantum Efficiency (QE, $\eta$, [0,1]): the probability of whether a photon, which enters the sensor finally generates an electron in the photo-electric layer
<ul>
<li>Transmission (coating and material absoption)</li>
<li>Geometry of microlens and the entire light path</li>
<li>$\lambda$</li>
<li>Angle of incidence</li>
<li>Numerical aperture of the incident light</li>
</ul>
</li>
<li>Deep trench isolation: walls are built between the pixels enhancing the QE and reducing <strong>cross-talk</strong></li>
<li>Stacking technology: <strong>light sensitive</strong> rear illuminated photodiode array is separated from the electronics (back-side illuminated sensors, i.e. BSI, higher sensitivity due to <strong>shorter</strong> and <strong>undisturbed</strong> path)</li>
</ul>
<h2 id="image-processing">Image processing</h2>
<ul>
<li>Create the most natural image possible of the object; enhance in terms of contrast or color rendition</li>
<li>Computational imaging
<ul>
<li>Combines several images from the same or different camera modules (3D sensors etc.)</li>
<li>Encoded or phase-distorted apertures can be deployed in combination with deconvolution (e.g. extend the depth of field i.e. EDoF)</li>
<li>3D acquisition and image recognition; AR</li>
</ul>
</li>
<li>Pipeline: noise reduction, linearization, shading correction, white balancing; demosaicing, distortion correction, to sRGB, gamma correction, sharpening, jpeg compression (more details can be found $\rightarrow$ <a href="/../notes/cs231m" >Course Notes/CS231m</a>)
<ul>
<li>Photoconversion curve is fitted to an 8-bit brightness scale by a tone value curve</li>
<li>Greater sharpening to medium or low spatial frequencies (human are particularly responsive); Moderately on high-frequency areas (This limits for complex motifs for low-light shots)</li>
<li>Different tone value curves are generally used for different photographed scenes to optimize the image (contrast, brightness, color)</li>
</ul>
</li>
</ul>
<h2 id="noise-and-noise-reduction">Noise and noise reduction</h2>
<ul>
<li>Small pixels: more sensitive to light; lower SNR; lower FWC $\rightarrow$ smaller dynamic range</li>
<li>SPCs compensated by image processing and sensor technology (e.g. deep trench isolation, binning, dual conversion gain, and higher quantum efficiency)</li>
<li>Noise from electrons is minor in CIS (contact image sensor) in good lighting conditions; Photon noise dominates for all cameras</li>
<li>Software-based noise reduction irrevocably smooth out small image details (i.e. resolution smaller, structures with less contrast $\rightarrow$ human skin appear unnatural)</li>
</ul>
<h2 id="focusing">Focusing</h2>
<ul>
<li>Depth of field of SPC lenses is significantly greater than that of full-frame camera lenses</li>
<li>Previous rule of thumb: for SPCs object distances of &gt;1 m, no focusing is required</li>
<li>For high-end standard wide-angle lens FOV = 75$\deg$, $\Theta_{im} = 12$ mm, $f = 7.82$ mm, f-number $K = 1.7$, then the hyperfocal distance is $4.5$ m. $\rightarrow$ if the lens is focused to an object distance of 4.5 m, the image is sharp for all object distances between 2.24 m to infinity</li>
<li>Focusing range and accuracy &ndash; key parameters
<ul>
<li>Focusing accuracy: no. of depth rranges within the complete focusing range</li>
<li>1st depth range: 2.25 m to infinity</li>
<li>2nd depth range: 2.25 m set as far distance, calculate the lower limit of the depth range
<ul>
<li>$s_{F, hyp} = h = 4.5 = \frac{f^{2}}{Kr\Theta_{im}}$</li>
<li>Realtive PSF spot size for sharp resulting image: $r = 1/1500$</li>
<li>$s_{near} = \frac{s_{F}}{1+s_{F}/h}$</li>
<li>$s_{far} = \frac{s_{F}}{1-s_{F}/h}$</li>
<li>Index $j$: successive depth ranges; $s_{near, j} = s_{far, j+1}$, with $s_{F, 0} = h = 4.5$, so $s_{F, 1} = h/3$&hellip;$s_{F, j} = h/(2j + 1)$ for adjacent depth ranges</li>
<li>The depth range quickly decreases: $s_{F, 3} = 0.9$ m (-15 cm, +23 cm) to the foreground and background is still sharp</li>
<li>Focusing distance $s_{F, 23} = 0.1$ m (-22 $\mu$ m, +23 $\mu$ m)</li>
<li>Depth ranges in terms of magnification instead of $s_{F}$: $m_{F} = \frac{s_{F}&rsquo;}{s_{F}} = \frac{f}{s_{F} + f}$ with $s_{F}&raquo;f \rightarrow m_{F}\approx \frac{f}{s_{F}}$</li>
<li>The depth range number $j$ linearly related to the magnification $m_{F} = (2j+1)\frac{f}{h}$</li>
<li>With minimum optical distance $s_{F, j} = s_{MOD} = h/(2j + 1)$, total number of depth ranges $J = \frac{h}{2S_{MOD}}$; SPC lens $J = \frac{4500mm}{2\cdot 100mm}\approx 23$</li>
</ul>
</li>
<li>3 focus steps per depth range $\rightarrow$ $3J\approx70$ focus postions; With total movement of lens actuator $\approx$ $0.28$ mm $\rightarrow$ positioning accuracy $0.28/70=4$ $\mu$ m $\rightarrow$ focusing accuracy $\Delta m_{F, acc}\approx \frac{f}{3h} = \frac{K}{2250\tan{(FOV/2)}}\approx 0.001$</li>
<li>Focusing accuracy depends on f-number $K$ and FOV according to the equation; also depend on actual size of the lens</li>
</ul>
</li>
<li>Front cameras: fixed focus, depth map cameras with a larger f-number providing greater depth of field</li>
<li>Focusing: autofocus system, modification of the optical imaging system for movement, mechatronic implementation of the drive</li>
</ul>
<h3 id="autofocus-methods-contrast-and-phase-detection-more-details-can-be-found-rightarrow-other-notes3aothernotes3a">Autofocus methods: Contrast and phase detection (more details can be found $\rightarrow$ <a href="/../othernotes/3a" >Other Notes/3A</a>)</h3>
<ul>
<li>Phase detection on reflex cameras: &ldquo;phase contrast measurement&rdquo;; with a mirror attached to the beam splitter and a separate image sensor, the deviation from the focus is quantitatively determined using triangulation
<ul>
<li>Pros: acquire optimal focal point exactly from the target shift from a single measurement</li>
<li>Cons: need folding mirror between lens and image plane (required space, larger lenses)</li>
</ul>
</li>
<li>Contrast detection in digital age: in the read-out image, until contrast is at its maximum
<ul>
<li>Pros: compact design</li>
<li>Cons: slow, need multiple measurements; swing back and forth; &ldquo;bad direction move&rdquo;; error-prone or fails for low-contrast objects (especially low-light conditions)</li>
</ul>
</li>
<li>SPC Phase detection auto focus (PDAF)
<ul>
<li>contrast detection $\rightarrow$ phase contrast pixels $\rightarrow$ dual pixel AF</li>
<li>Phase contrast pixels: &ldquo;light version&rdquo; of SLR phase contrast; some masked pixels at 4 different orientations (5-10%) are used as focus pixels (not available as normal senosr pixels); left &ldquo;blind spots&rdquo; to be interpolated</li>
<li>Dual pixel AF
<ul>
<li>Under a common mirolens have two separate neighbored pixels (&ldquo;photodiode twin&rdquo;); does not suffer from blind spots</li>
<li>Towards outer field regions, PDAF becomes problematic due to the oblique incidence of light (35 $\deg$ in the image corners)</li>
<li>Cooptimization of pixel architecture, microlens design, and data processing can be supported by ray-tracing and wave-calculation-based image simulations</li>
</ul>
</li>
<li>ToF or lidar for active distance measuring systems to improving spatial resolution</li>
<li>Taking a whole stack of pictures (&ldquo;live image&rdquo; or &ldquo;motion still&rdquo;), once the object is in focus, switch to high resolution, save the image, and delete the image stack from the buffer.</li>
</ul>
</li>
</ul>
<h3 id="optical-system-changes-focus-position">Optical system changes focus position</h3>
<ul>
<li>Entire lens (&ldquo;total lens focusing&rdquo;) or partial individual optical groups (&ldquo;floating element focusing&rdquo;, adpoted for SPC)</li>
<li>Deforming lens with liquid lenses (electrowetting); A pair of aspherical components that can be moved laterally</li>
<li>Focusing distance: $-\frac{1}{s}+\frac{1}{s&rsquo;} = \frac{1}{f}$, where $s$ is the object distance, $s&rsquo;$ is the image distance from the front or rear principal plane</li>
<li>The lens is moved forward in order to focus on an object that is closer to the lens</li>
<li>The distance required for focusing scales almost quadratically with the focal length, not linear: $\Delta s&rsquo;<em>{MOD} = s&rsquo;</em>{MOD} - f = -\frac{f}{s_{MOD}+f}$</li>
<li>Even with same f, but differently sized image sensors have different distances to move for focusing</li>
<li>With magnification equation $m = \frac{s&rsquo;}{s} = \frac{f}{s+f}$, the focus offset $\Delta s&rsquo;_{MOD} = -mf$, $\rightarrow$ a lens with focal length f, the <strong>focus movement</strong> distance directly scales with <strong>magnification</strong></li>
<li>DSLR: the typical close distance magnification is about 1:10; SPC: a close distance of 100 mm magnification is about 1:15</li>
<li>The longer focal lengths, more space is required for the focus movement</li>
<li>Preferably with 2 moving groups to achieve sufficient image performance</li>
</ul>
<h3 id="focusing-mechanism-voice-coil-motors-vcm-and-other-concepts">Focusing mechanism: Voice coil motors (VCM) and other concepts</h3>
<ul>
<li>VCMs
<ul>
<li>Almost exclusively used in SPCs as drives for the focus movement of the lens</li>
<li>Structure: AF VCM and housing attached on the image sensor plane, the outer case has folded connector cable and gimbal vois coils</li>
<li>Issues with predictability in its location ($10 \mu$ m) due to <strong>directional hysteresis</strong> (image depth of field, temperature, coil resistance variation) $\rightarrow$ open-loop control before correct focus</li>
</ul>
</li>
<li>Alternative actuator types: stepper motors, piezo motors, MEMS, EDoF</li>
<li>EDoF (extended depth-of-focus) by computational imaging
<ul>
<li>3rd order profile aspherical extends the DoF at the expense of contrast</li>
<li>Recovered by deconvolution with a priori data of the phase mask</li>
<li>2 DoF extension, which in turn translates to a significant extension of DoF in object distance equal to minimum optical distance (MOD = 30 cm). Less than a standard AF with barrel shift (MOD $\approx 10$ cm)</li>
<li>Moving the lens to a desired focus position is obsolete</li>
<li>Cons: noise from decomvolution (especially in low-light condition, can cause unrecoverable contrast)</li>
</ul>
</li>
</ul>
<h2 id="image-stabilization">Image stabilization</h2>
<ul>
<li>During image exposure, imageing performance limited by human hand-shaking, low light conditions, etc. due to <strong>intrinsically low etendue</strong> and a <strong>lower FWC</strong> $\rightarrow$ longer exposure ($\times$ squared crop factor, approx 50 for same f-number), ISO $\uparrow$ introducing additional noise</li>
<li>The weight of SLR also helps reduce hand shaking</li>
<li>Electronic image stabilization (EIS) and optical image stabilization (OIS) used simultaneously
<ul>
<li>Front cameras may not contain OIS</li>
<li>EIS: Hand shaking as measured with on-board gyroscope and accelerometer; image frame is slightly cropped and conpensated by frame shift</li>
<li>EIS cons
<ul>
<li>Cannot help in movement during one-frame exposure, would result in blur $\rightarrow$ can be reduced by deconvolution of the intergral PSF (computationally intensive task)</li>
<li>Limited to FOV portion since working on reduction of FOV $\rightarrow$ larger hand-shaking amplitudes result in residual errors</li>
</ul>
</li>
<li>OIS: residual aberrations since the degrees of freedom used in the OIS system are not the same (<strong>pitch, yaw, roll</strong>)</li>
</ul>
</li>
</ul>
<h3 id="hand-shaking-and-image-blur">Hand-shaking and image blur</h3>
<ul>
<li>Human tremor: involuntary oscillatory movement of body parts directly caused by muscles contracting and relaxing repetitively</li>
<li>The key for EIS and OIS is the availability of on-board MEMS-based (micro-electro mechanical systems, gyroscope, accelerometer, etc.) miniature sensors.</li>
<li>With differential capacities in comb-drive actuator design, the sensitivities of the sensors are <strong>linear</strong> and <strong>high-voltage sensitivity</strong>, leading to <strong>low power</strong> consumption</li>
<li>6 degree of freedom: 3 transitions (<strong>x, y, z</strong>), 3 angular directions (<strong>pitch, yaw, roll</strong>) denoted $\theta_{x}$, $\theta_{y}$, $\theta_{z}$
<ul>
<li>Image point $(x&rsquo;, y&rsquo;)$, object point $(x, y)$</li>
<li>$dx&rsquo; = f&rsquo;d\theta_{y} + \frac{f&rsquo;}{s}dx + yd\theta_{z}$</li>
<li>$dy&rsquo; = f&rsquo;d\theta_{x} + \frac{f&rsquo;}{s}dy + xd\theta_{z}$</li>
<li>1st components: pitch and yaw, measured by gyroscope</li>
<li>2nd components: measured by accelerometer</li>
<li>3rd components: roll (azimuthal-oriented blur, increases linearly in a radial direction from the image center)</li>
<li>$\theta_{x}$, $\theta_{y}$, $\theta_{z}$ are simliar in amplitude $\rightarrow$ the roll-induced blur in the image corner is purely geometrically related to the yaw and pitch induced blur offsets (e.g. $\tan{(FOV/2)} = \Theta_{im}/(2f&rsquo;)$, lens with FOV of $\75\deg$ has a factor of about 0.75 smaller blur in the image corner compared to yaw and pitch induced blur)</li>
</ul>
</li>
<li>Most OIS systems are not able to compensate for this component (&ldquo;usual&rdquo; situation is &lt; $1\deg$; most OIS is $0.5$-$1\deg$)</li>
<li>&ldquo;Normal&rdquo; object distances (e.g. 1m), angular dominates; close-distance, decenter components dominate; at about $0.3$ m the contributions are comparable</li>
<li>Typical hand-shaking blur is not a purely random walk, containing some <strong>systematic components</strong></li>
<li>SPCs interior images: exposure time 1/10 s; night scenes: 1 s; image blur is in the order of 10 pixels</li>
</ul>
<h3 id="optical-image-stabilization-implementations">Optical image stabilization implementations</h3>
<ul>
<li>OIS systems: barrel decenter (most common); image sensor decenter (in SLR referred to as body image stabilization BIS); gimbal</li>
<li>In a first-order approximationd a <strong>tilt and shift</strong> of the barrel is <strong>equvalent</strong> $\rightarrow$ correction of a hand-shaking tilt or decenter</li>
<li>Barrel decenter: compensating movement of the entire lens barrel in x, y with a voice coil motor; some systems, barrel is tilted
<ul>
<li>Stiffness of the springs: correctable amplitude frequency distribution (e.g. very stiff spring $\rightarrow$ high-freq corrections but smaller amplitudes at low freq)</li>
<li>Yoke: tight adjustment incresases friction and hysteresis; loose adjustment may lead to parasitic tilts</li>
</ul>
</li>
<li>Image sensor decenter: iPhone 12 Pro, image sensor is actively moved</li>
<li>Gimbal syetem: vivo X51,tilts the complete barrel together with the fixed-to-barrel image sensor</li>
</ul>
<h2 id="dynamic-range">Dynamic range</h2>
<h3 id="hdr-imaging">HDR imaging</h3>
<ul>
<li>The limited dynamic range of image sensors in relation to the range of irradiances in a scene
<ul>
<li>Typical outdoor scenes: 9-12 EV, rarely above 14 EV</li>
<li>Strong light source: &gt; 20 EV</li>
<li>High-end, full-frame digital consumer cameras: DR $\approx$ 14-15 EV (DxO Mark)</li>
<li>SPCs: DR $\approx$ 10-12 EV (DxO Mark)</li>
</ul>
</li>
<li>DR as low ISO sensitivity: DR decreases as ISO increases (0.6-1.1 EV per ISO step, e.g. ISO 6400 DR 6-9)</li>
<li>Calculate HDR images from a sequence of different exposoure times</li>
<li>The tone values of these HDR recordings, which extend over a very large value range (e.g. 16 bits) are reduced to a much smaller brightness range (e.g. 8 bits)</li>
<li>Scene recorded several times with different exposure times and combined for the extended dynamic range according to the brightness range shown</li>
<li>Unsuitable for fast-moving subjects and more tend to loss of resolution due to hand shaking</li>
<li>Mitigated with multicell sensors (exposure times of different lengths to be parallelized with different pixel clusters, with <strong>pixel binning</strong>)</li>
</ul>
<h3 id="lens-flare-and-ghosts">Lens flare and ghosts</h3>
<ul>
<li>When the sum or another very birght light source is in or just outside the frame, DR &gt; 25 or 30 EV</li>
<li>Lens quality is crucial: residual light reflection on lens surfaces from a bright light source may superpose or cover up parts of the image; even for multilayer coated surface (&lt;0.2% reflectivity)</li>
<li>Double reflex: with image sensor 5% reflectivity $\rightarrow$ 1/10000 weaker $\rightarrow$ -14 EV from the light source; but the maximum irradiance within normal scene is about 15 EV smaller than the light source</li>
<li>No. of lens surface-surface reflections inside lens with $n$ surfaces: $n + (n - 1) + \cdot \cdot \cdot + 2 + 1 = \frac{1}{2}n(n + 1)$; all surfaces including in and between micro lens, IR filters, etc.</li>
<li>Shapes: Caustics or crescent-shaped (large aperture); egg-shaped (smartphone); often purple (from higher coating reflectivity in blue and red)</li>
<li>Every scene with a large dynamic range</li>
<li>Good hardware (i.e. AR coatings, straylight-blocking rings), the straylight performance can be optimized on optical design phase to avoid in-focus ghosts (modifications on the optical surface shapes, opto-mechanical layout of the system)</li>
<li>May appear inside a very small portion of the full FOV (distinct local differrences in reflection direction at the wiggly lens surfaces; surfaces exceeding the total reflection angle)</li>
<li>Testing in the lab: local variation of straylight can easily be observed when rotating the lens relative to a very bright light surface</li>
</ul>
<h2 id="portrait-mode">Portrait mode</h2>
<ul>
<li>Subject recognition $\rightarrow$ portraits of people, portraits of pets, etc; context-by-context basis; shape of edges (face contours); combined with 3D acquisition to improve depth maps</li>
<li>Computationally blurring out-of-focus regions according to the depth map data; depth dependent PSF stored in the memory; dependent on both depth and the selected focus to resemble the real images</li>
<li>The depth map should accurately reproduce the scene in all its details</li>
<li>The depth of field of an PSC is many times greater than that of the DSLR due to the much smaller image format
<ul>
<li>Relative size of the circle of confusion (diameter of the PSF): $\Theta_{rel, spot , \inf} = \frac{\Theta_{EP}}{\Theta_{ob, portrait}} = \frac{\Theta_{EP}}{700 mm}$</li>
<li>SPC equivalent focal length of 85 mm requires 85 mm / crop factor = 8.8 mm, resulting in entrance pupil diameter $\Theta_{EP} = 8.8 mm / 3 \approx 3 mm \rightarrow \Theta_{rel, spot , \inf} \approx 0.42%$</li>
<li>DSLR portrait lens with entrance pupil diameter of $85mm/1.4\approx 60mm$ and a relative background blur spot diameter of $\Theta_{rel, spot , \inf} \approx 8.7%$, which is 20 times larger for SPC</li>
</ul>
</li>
</ul>
<h3 id="3d-depth-acquisition-technology">3D depth acquisition technology</h3>
<ul>
<li>Can be deterrmined stereoscopically on the common image portion of the cameras</li>
<li>Distance to objects can be determined for the directional component of the neighboring cameras and not in the direction perpendicular to it $\rightarrow$ with more cameras, the disparity can be captured along different directions, improves the quality of the depth map</li>
<li>Multi-cameras: occluded area reduced, disparity more robust</li>
<li>Classic multiview stereo processing is <strong>computationally expensive</strong>, challenging on mobile; involving feature extraction, feature matching, camera calibration and pose estimation, depth estimation, surface reconstruction, and texturing</li>
<li>Lightfield imaging
<ul>
<li>Conventional lens is combined with a micro lens array with a much smaller focal length</li>
<li>Micro lens array is positioned closely in front of an image sensor</li>
<li>The raw image consists of many small partly overlapping fields of view of a scene</li>
<li>Correlation analysis between adjacent fields of view $\rightarrow$ dispartiy and depth</li>
<li>Smaller focal length results in small achievable disparities (&lt;1%) and inferior higher depth resolution</li>
<li>Extended depth of field with different focal lengths</li>
</ul>
</li>
<li>Disparity calculation
<ul>
<li>Object points distances $s_{1}$, $s_{2}$; base distance between the cameras $b$; image positions $y_{1}$, $y_{2}$</li>
<li>$\frac{s_{1}}{b} = \frac{f}{y_{1}}$</li>
<li>$\frac{s_{2}}{b} = \frac{f}{y_{2}}$</li>
<li>Disparity $d = y_{2} - y_{1} = fb(\frac{1}{s_{2}} - \frac{1}{s_{1}})$</li>
<li>With $f = \frac{y_{max}}{\tan{(FOV/2)}}$, the disparity relative tot he image frame $\frac{d}{y_{max}} = \frac{b}{\tan{(FOV/2)}}(\frac{1}{s_{2}} - \frac{1}{s_{1}})$</li>
<li>Assume $s_{1}\rightarrow \inf$, $s = \frac{by_{max}}{d\tan{(FOV/2)}}$</li>
<li>$b$, $d$, $y_{max}$, FOV are known from lens, calibrated camera (positioning, axis-orientation)</li>
<li>Typical SPC $b$: 1-2 cm</li>
<li>For normal tele lens, a relative disparity of $d/y_{max} = 0.354$ (3.54% of the distance between image center and corner), which is about portrait distance for FOV $= 44\deg$; 2.5 m object distance, 20 pixels; 10 distance disparity, 2 pixels</li>
<li>Depth resolution decreases as distance from the lens increases</li>
<li>Depth resolution depends on the depth of field and actual focus position</li>
<li>For unstructured objects (e.g. clear blue sky), machine learning</li>
<li>Quality of the depth map highly dependent on the contrast and light intensity, spectral distribution of the illumination in the scene</li>
</ul>
</li>
<li>Incorrect depth data unnatural blurring at the edge between foreground and background</li>
<li>Face recognition requires 3D depth data: structured light camera system, ToF sensors (time of flight, range imaging camera, lower power light source infrared light; LiDAR: laser beams)
<ul>
<li>Light source: VCSEL (vertical cavity surface-emitting laser) at a power of ~150mW in near IR light</li>
<li>Modulated light for deduce temporally pulsed or continuous waved</li>
</ul>
</li>
</ul>
<h3 id="simulation-of-lens-bokeh-camera-3d-point-spread-function">Simulation of lens bokeh: Camera 3D point spread function</h3>
<ul>
<li>Equation of the diameter of the geometrical PSF (&ldquo;circle of confusion&rdquo;): $\Theta_{rel, spot} = \frac{f^{2}}{\Theta_{im}K}\frac{|S_{F} - s|}{s_{F}s}$ for &ldquo;normal imaing&rdquo; without considering macro distances $\rightarrow$ when object distance $s &lt; s_{F}$, $s\uparrow$, circle of confusion $\Theta_{rel, spot}\downarrow$; Further move the object, $s\uparrow$, circle of confusion $\Theta_{rel, spot}\uparrow$</li>
<li>Bokeh (Êöà„Åë/„Éú„Ç±)
<ul>
<li>Non-uniform intensity distribution within an out-of-focus hightlight source is due to <strong>lens aberrations</strong>
<ul>
<li><strong>Overlapping</strong> out-of-focus highlights</li>
<li>&ldquo;<strong>Cat&rsquo;s-eye bokeh</strong>&rdquo; due to <strong>vignetting</strong> usually for high-aperture lenses (<strong>lens&rsquo; field stops</strong> in the front and rear part of camera lenses)</li>
<li>&ldquo;<strong>Edgy bokeh</strong>&rdquo; due to <strong>iris stop</strong> (diaphragm with blades)</li>
<li><strong>Softar bokeh</strong> (softar filter scatters light out of nominal light path)</li>
<li>&ldquo;<strong>Donut bokeh</strong>&rdquo; (spherical aberration, red color fringe due to chromatic aberration) due to overcorrected low-order spherical aberration (redistribution of intensity in the presence of aberrations)</li>
<li>&ldquo;<strong>Christmax ball bokeh</strong>&rdquo; (spherical aberration)</li>
<li>&ldquo;<strong>Fine structured bokeh</strong>&rdquo; including &ldquo;<strong>onion ring bokeh</strong>&rdquo; (optical manufacturing induced residual surface deformations)</li>
</ul>
</li>
<li>light intensity: not a problem when not exceeding DR; exceeding DR, irradiance is physically <strong>redistributed</strong> inversely proportional to the surface area of the out-of-focus spot $\rightarrow$ synthetic bokeh of an SPC spot brigihtness must be <strong>guessed</strong></li>
</ul>
</li>
</ul>
<h3 id="portrait-look-a-quality-evaluation">Portrait look: a quality evaluation</h3>
<ul>
<li>Synthetic bokeh: depth map quality, inferior low-light, reduced DR</li>
<li>Challenging in low-light: noisy face due to the low ambient light; unpleasant differences between artifitial smoothed background and noisy face; overexposed bright colored highlight on the background loss its DR appearing white $\rightarrow$ computed blurred background incorrectly displayed as white (unexpected larger defocusing area with less intensity than saturation value of the DR, not uniform, failed bokeh)</li>
<li>DxO computational bokeh test setup
<ul>
<li>Critical objects for high-resolution depth acquisition in topologically enclosed areas like holes or tiny eyelets</li>
<li>Structured stipes which are placed diagonally into the depth for checking natrually and continuously sharpness transition in the depth</li>
</ul>
</li>
<li>Criteria based on the ideal of the natural bokeh
<ul>
<li>Depth map quality:
<ul>
<li>Subject background segmentation</li>
<li>Repeatability</li>
<li>Blur gradient smoothness (also 3D DSF model)</li>
</ul>
</li>
<li>3D PSF model
<ul>
<li>Bokeh shape</li>
<li>Equivalent aperture</li>
</ul>
</li>
<li>&ldquo;Noise&rdquo; uniformity: Noise consistency (face noise and background smoothness)</li>
</ul>
</li>
</ul>
<h2 id="image-performance-specification-and-test">Image performance specification and test</h2>
<p>Lab evaluation during <strong>R&amp;D</strong>; qualification in <strong>mass production</strong> (mentioned in 8.3, active optical assembly); qualification of the <strong>image quality</strong>, including signal and image processing in the SP</p>
<h3 id="lab-evaluation-during-rd-objectively-checking-measurable-optical-and-sensor-properties">Lab evaluation during R&amp;D: <strong>Objectively</strong> checking measurable optical and sensor properties</h3>
<ul>
<li>Tests and standardized test charts
<ul>
<li>Resolution and contrast: CIPAA resolution (wedge) test chart, OECF/noise chart with 20 gray patches</li>
<li>Color reproduction: X-Rite color checker digital SG</li>
<li>Field of view (FOV) and distortion: 19/14-grid test chart</li>
<li>Dynamic range</li>
<li>Auto exposure (AE)</li>
<li>Autofocus (AF)</li>
<li>Auto white balance (AWB)</li>
<li>Lens shading</li>
<li>Color shading</li>
<li>Flare: Flare target, round dots forming a cross</li>
<li>Ghosts</li>
</ul>
</li>
<li>Other tests and inspections: material tests, dust and environmental tests, shock and vibration, continuous run, lifetime, electromagnetic compatibility, electronic tests (software and sensor interfaces to the camera, drives for VCM and OIS)</li>
</ul>
<h3 id="evaluation-of-image-quality-in-the-imaging-pipeline">Evaluation of image quality in the imaging pipeline</h3>
<ul>
<li>Subjective quality tests: test motifs including natural objects, special image arrangements, and people</li>
<li>As objective as possible: <strong>trained person</strong> according to precisely specified <strong>test</strong> and <strong>evaluation procedures</strong> (lighting, viewing time, etc.)</li>
<li>Image processing software takes place practically until the device is sold $\rightarrow$ assess as early as possible</li>
<li>No recognized stantards forr the overall evaluation of a camera system</li>
<li>DxOMark (broad public perception, commercial), VCX (nonprofit, consortium)</li>
</ul>
<h2 id="smartphone-camera-interface-with-telescopes-microscopes-and-accessory-lenses">Smartphone camera interface with telescopes, microscopes, and accessory lenses</h2>
<h2 id="summary-and-outlook">Summary and outlook</h2>
<h2 id="references">References:</h2>
<ol>
<li>Walasek-Hoehne, B., K. Hoehne, and R. Singh. &ldquo;Video Cameras used in Beam Instrumentation&ndash;an Overview.&rdquo; arXiv preprint arXiv:2005.04977 (2020).</li>
<li><a href="https://www.edmundoptics.com/knowledge-center/application-notes/imaging/limitations-on-resolution-and-contrast-the-airy-disk/"  target="_blank" rel="noopener" >https://www.edmundoptics.com/knowledge-center/application-notes/imaging/limitations-on-resolution-and-contrast-the-airy-disk/</a></li>
<li>Kap≈Çonek, Wojciech, et al. &ldquo;Optical profilometer with confocal chromatic sensor for high-accuracy 3D measurements of the uncirculated and circulated coins.&rdquo; Journal of Mechanical and Energy Engineering 2 (2018).</li>
<li><a href="https://www.photometrics.com/learn/camera-basics/binning"  target="_blank" rel="noopener" >https://www.photometrics.com/learn/camera-basics/binning</a></li>
<li>Tournier, A., et al. &ldquo;Pixel-to-pixel isolation by deep trench technology: application to CMOS image sensor.&rdquo; Proc. Int. image sensor workshop. 2011.</li>
<li><a href="https://www.dpreview.com/articles/1570070253/what-is-dual-gain-and-how-does-it-work"  target="_blank" rel="noopener" >https://www.dpreview.com/articles/1570070253/what-is-dual-gain-and-how-does-it-work</a></li>
<li><a href="https://www.photometrics.com/learn/imaging-topics/quantum-efficiency"  target="_blank" rel="noopener" >https://www.photometrics.com/learn/imaging-topics/quantum-efficiency</a></li>
<li><a href="https://www.pveducation.org/pvcdrom/solar-cell-operation/quantum-efficiency"  target="_blank" rel="noopener" >https://www.pveducation.org/pvcdrom/solar-cell-operation/quantum-efficiency</a></li>
<li><a href="https://optcorp.com/blogs/telescopes-101/the-basic-telescope-types"  target="_blank" rel="noopener" >https://optcorp.com/blogs/telescopes-101/the-basic-telescope-types</a></li>
</ol>

  </article>

</div>


  
<script type="text/javascript" src="/main.js" defer></script>


</body>

</html>